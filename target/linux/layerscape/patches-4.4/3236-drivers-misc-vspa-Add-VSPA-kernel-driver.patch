From b56cc159438154bd035e299f9b3f8ecbe0b59f1b Mon Sep 17 00:00:00 2001
From: Rajesh Bhagat <rajesh.bhagat@nxp.com>
Date: Mon, 10 Apr 2017 09:24:03 +0530
Subject: [PATCH 03/26] drivers: misc: vspa: Add VSPA kernel driver

Signed-off-by: Abhimanyu Saini <abhimanyu.saini@nxp.com>
Signed-off-by: Rajesh Bhagat <rajesh.bhagat@nxp.com>
---
 Documentation/devicetree/bindings/vspa.txt     |   25 +
 arch/arm64/boot/dts/freescale/fsl-la1575a.dtsi |   97 +
 drivers/misc/Kconfig                           |    8 +
 drivers/misc/Makefile                          |    1 +
 drivers/misc/vspa/Makefile                     |    1 +
 drivers/misc/vspa/event.c                      |  438 +++++
 drivers/misc/vspa/main.c                       | 2510 ++++++++++++++++++++++++
 drivers/misc/vspa/sys.c                        |  230 +++
 drivers/misc/vspa/vspa.h                       |  426 ++++
 include/uapi/linux/vspa.h                      |  241 +++
 10 files changed, 3977 insertions(+)
 create mode 100644 Documentation/devicetree/bindings/vspa.txt
 create mode 100644 drivers/misc/vspa/Makefile
 create mode 100644 drivers/misc/vspa/event.c
 create mode 100644 drivers/misc/vspa/main.c
 create mode 100644 drivers/misc/vspa/sys.c
 create mode 100644 drivers/misc/vspa/vspa.h
 create mode 100644 include/uapi/linux/vspa.h

diff --git a/Documentation/devicetree/bindings/vspa.txt b/Documentation/devicetree/bindings/vspa.txt
new file mode 100644
index 0000000..89a07c9
--- /dev/null
+++ b/Documentation/devicetree/bindings/vspa.txt
@@ -0,0 +1,25 @@
+Kernel driver landshark
+=======================
+
+Supported chips:
+Freescale LA1575A, Medusa, DFE4400
+
+Device Tree Entry
+-----------------
+
+vspa0: vspa@0xa000000 {
+	compatible = "fsl,vspa";
+	reg = <0 0xa000000 0 0x800>,
+	    <0 0xa090000 0 0x180>;
+
+	/* VSPA ATU configurations */
+	vspa_atu_tar = <0x81 0xf0000000>;
+	vspa_atu_bar = <0xc 0x40000000>;
+
+	dbgregstart = <0x7 0x4400000>;
+	dbgreglen = <0x1000>;
+
+	vspadev-id  = <0>;
+	interrupts = <0 232 0x4>;
+	status = "okay";
+}
diff --git a/arch/arm64/boot/dts/freescale/fsl-la1575a.dtsi b/arch/arm64/boot/dts/freescale/fsl-la1575a.dtsi
index 5e22f8e..57008e4 100644
--- a/arch/arm64/boot/dts/freescale/fsl-la1575a.dtsi
+++ b/arch/arm64/boot/dts/freescale/fsl-la1575a.dtsi
@@ -541,6 +541,103 @@
 	};
 
 
+	aiop-wifi-phy {
+		compatible = "simple-bus";
+		#address-cells = <2>;
+		#size-cells = <2>;
+		ranges;
+
+		vspa_atu: atu@a090000 {
+			/* VSPA ATU has 7 + 1 translation
+				Windows */
+			reg = <0 0xa090000 0 0x180>;
+			/* Number of ATU windows to be used */
+			atu_win_cnt = <1>;
+			/* ATU Windows Entry Format:
+			* Maximum Window size supported by ATU is 2GB
+			* atu_win_<n> =
+			*	< <64 Bit SoC Address> <Window Size> >
+			*/
+
+			/*	Value Window Size
+			*	00000 64 KB
+			*	00001 128 KB
+			*	00010 256 KB
+			*	00011 512 KB
+			*	00100 1 MB
+			*	00101 2 MB
+			*	00110 4 MB
+			*	00111 8 MB
+			*	01000 16 MB
+			*	01001 32 MB
+			*	01010 64 MB
+			*	01011 128 MB
+			*	01100 256 MB
+			*	01101 512 MB
+			*	01110 1 GB
+			*	01111 2 GB
+			*	1xxxx Reserved
+			*/
+			atu_win_1 = <0x81 0xf0000000 0x0 0x0c>;
+		};
+
+		vspa0: vspa@0xa000000 {
+			compatible = "fsl,vspa";
+			reg = <0 0xa000000 0 0x800>;
+
+			atu = <&vspa_atu>;
+
+			dbgregstart = <0x7 0x4400000>;
+			dbgreglen = <0x1000>;
+
+			vspadev-id  = <0>;
+			interrupts = <0 232 0x4>;
+			status = "okay";
+		};
+
+		vspa1: vspa@0xa002000 {
+			compatible = "fsl,vspa";
+			reg = <0 0xa002000 0 0x800>;
+
+			atu = <&vspa_atu>;
+
+			dbgregstart = <0x7 0x4401000>;
+			dbgreglen = <0x1000>;
+
+			vspadev-id  = <1>;
+			interrupts = <0 233 0x4>;
+			status = "okay";
+		};
+
+		vspa2: vspa@0xa004000 {
+			compatible = "fsl,vspa";
+			reg = <0 0xa004000 0 0x800>;
+
+			atu = <&vspa_atu>;
+
+			dbgregstart = <0x7 0x4402000>;
+			dbgreglen = <0x1000>;
+
+			vspadev-id  = <2>;
+			interrupts = <0 234 0x4>;
+			status = "okay";
+		};
+
+		vspa3: vspa@0xa006000 {
+			compatible = "fsl,vspa";
+			reg = <0 0xa006000 0 0x800>;
+
+			atu = <&vspa_atu>;
+
+			dbgregstart = <0x7 0x4403000>;
+			dbgreglen = <0x1000>;
+
+			vspadev-id  = <3>;
+			interrupts = <0 235 0x4>;
+			status = "okay";
+		};
+	};
+
 	memory@80000000 {
 		device_type = "memory";
 		reg = <0x00000000 0x80000000 0 0x80000000>;
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index 03bf175..416d040 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -554,6 +554,14 @@ config VEXPRESS_SYSCFG
 	  bus. System Configuration interface is one of the possible means
 	  of generating transactions on this bus.
 
+config VSPA
+	tristate "VSPA kernel Driver"
+	depends on ARCH_LAYERSCAPE
+	default y
+	help
+	  The VSPA kernel driver is the part of user space library providing
+	  primitives for the VSPA driver in user space
+
 source "drivers/misc/c2port/Kconfig"
 source "drivers/misc/eeprom/Kconfig"
 source "drivers/misc/cb710/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index ec4d86a..ddf0776 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -57,3 +57,4 @@ obj-$(CONFIG_GENWQE)		+= genwqe/
 obj-$(CONFIG_ECHO)		+= echo/
 obj-$(CONFIG_VEXPRESS_SYSCFG)	+= vexpress-syscfg.o
 obj-$(CONFIG_CXL_BASE)		+= cxl/
+obj-$(CONFIG_VSPA)		+= vspa/
diff --git a/drivers/misc/vspa/Makefile b/drivers/misc/vspa/Makefile
new file mode 100644
index 0000000..8e9a7fa
--- /dev/null
+++ b/drivers/misc/vspa/Makefile
@@ -0,0 +1 @@
+obj-y +=	main.o sys.o event.o
diff --git a/drivers/misc/vspa/event.c b/drivers/misc/vspa/event.c
new file mode 100644
index 0000000..75d101a
--- /dev/null
+++ b/drivers/misc/vspa/event.c
@@ -0,0 +1,438 @@
+/*
+ * drivers/misc/vspa/event.c
+ * VSPA device driver
+ *
+ * Copyright (C) 2016 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/types.h>
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/list.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/wait.h>
+#include <linux/uaccess.h>
+#include <linux/signal.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/input.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_gpio.h>
+#include <linux/of_address.h>
+#include <linux/pid.h>
+#include <linux/mm.h>
+#include <linux/poll.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <linux/list.h>
+
+#include <uapi/linux/vspa.h>
+#include "vspa.h"
+
+/************************** Event Queue *******************************/
+
+static inline uint32_t events_present(struct vspa_device *vspadev)
+{
+	uint32_t mask = vspadev->event_list_mask;
+	struct event_queue *eq = &(vspadev->event_queue);
+
+	if (eq->idx_enqueue != eq->idx_dequeue) /* Queue is not empty */
+		mask |= vspadev->event_queue_mask;
+	return mask;
+}
+
+static inline int event_next_index(int curr)
+{
+	return (curr == (EVENT_QUEUE_ENTRIES - 1)) ? 0 : curr + 1;
+}
+
+void event_reset_queue(struct vspa_device *vspadev)
+{
+	int i;
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&vspadev->event_queue_lock, irqflags);
+	vspadev->event_queue_mask = 0;
+	vspadev->event_queue.idx_enqueue = 0;
+	vspadev->event_queue.idx_dequeue = 0;
+	vspadev->event_queue.idx_queued = 0;
+	spin_unlock_irqrestore(&vspadev->event_queue_lock, irqflags);
+
+	spin_lock(&vspadev->event_list_lock);
+	vspadev->event_list_mask = 0;
+	INIT_LIST_HEAD(&vspadev->events_free_list);
+	INIT_LIST_HEAD(&vspadev->events_queued_list);
+	for (i = 0; i < EVENT_LIST_ENTRIES; i++)
+		list_add(&vspadev->events[i].list,
+			 &vspadev->events_free_list);
+	spin_unlock(&vspadev->event_list_lock);
+}
+
+/* This routine is usually called from IRQ handlers */
+void event_enqueue(struct vspa_device *vspadev, uint8_t type,
+	uint8_t id, uint8_t err, uint32_t data0, uint32_t data1)
+{
+	struct event_queue *eq = &(vspadev->event_queue);
+	struct event_entry *er;
+	int next_slot;
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&vspadev->event_queue_lock, irqflags);
+
+	if (eq->idx_enqueue == eq->idx_dequeue) /* Queue is empty */
+		vspadev->event_queue_mask = 0;
+
+	next_slot = event_next_index(eq->idx_enqueue);
+
+	if (next_slot == eq->idx_dequeue) {	/* Queue is full */
+		er = &(eq->entry[eq->idx_queued]);
+		if (type > 7)
+			type = 0;
+		er->lost |= 1 << type;
+	} else {
+		er = &(eq->entry[eq->idx_enqueue]);
+		er->type  = type;
+		er->id    = id;
+		er->err   = err;
+		er->data0 = data0;
+		er->data1 = data1;
+		er->lost = 0;
+		eq->idx_queued = eq->idx_enqueue;
+		eq->idx_enqueue = next_slot;
+
+		vspadev->event_queue_mask |= 0x10 << type;
+		wake_up_interruptible(&vspadev->event_wait_q);
+	}
+	spin_unlock_irqrestore(&vspadev->event_queue_lock, irqflags);
+}
+
+void event_list_update(struct vspa_device *vspadev)
+{
+	struct event_list *ptr;
+	struct event_list *last;
+	struct event_queue *eq = &(vspadev->event_queue);
+	struct event_entry *er;
+	uint32_t mask = 1;
+	int reply_bd_idx;
+
+	if (eq->idx_enqueue == eq->idx_dequeue)
+		return;
+
+	spin_lock(&vspadev->event_list_lock);
+
+	if (list_empty(&vspadev->events_queued_list))
+		last = NULL;
+	else
+		last = list_last_entry(&vspadev->events_queued_list,
+				       struct event_list, list);
+		mask = 0;
+
+	while (eq->idx_enqueue != eq->idx_dequeue) {
+		/* TODO - check SPM for command error messages */
+		er = &(eq->entry[eq->idx_dequeue]);
+		/* coalese error events of the same type */
+		if (last &&
+		    last->type == ((0x10<<VSPA_EVENT_ERROR)|VSPA_EVENT_ERROR) &&
+		    er->type == VSPA_EVENT_ERROR &&
+		    er->id == last->id &&
+		    er->id == VSPA_ERR_WATCHDOG) {
+			last->data0 = er->data0;
+			last->data1++;
+			IF_DEBUG(DEBUG_EVENT)
+				pr_info("vspa%d: co %04X %02X %02X %08X %08X\n",
+					vspadev->id, last->type, last->id,
+					last->err, last->data0, last->data1);
+		} else {
+			if (list_empty(&vspadev->events_free_list)) {
+				mask = 0;
+				ERR("%d: Event queue overflowed\n",
+								 vspadev->id);
+				/* TODO - add lost events */
+				ptr = list_first_entry(
+						&vspadev->events_queued_list,
+						struct event_list, list);
+				reply_bd_idx = ptr->data1 >> 24;
+				if ((ptr->type == VSPA_EVENT_REPLY) &&
+				    (reply_bd_idx < BD_ENTRIES)) {
+					pool_free_bd(&vspadev->reply_pool,
+								reply_bd_idx);
+				}
+				list_move_tail(&ptr->list,
+						 &vspadev->events_free_list);
+			}
+			ptr = list_first_entry(&vspadev->events_free_list,
+					       struct event_list, list);
+			ptr->err   = er->err;
+			ptr->id    = er->id;
+			ptr->type  = (0x10 << er->type) | er->type;
+			ptr->data0 = er->data0;
+			ptr->data1 = er->data1;
+			IF_DEBUG(DEBUG_EVENT)
+				pr_info("vspa%d: up %04X %02X %02X %08X %08X\n",
+					vspadev->id, ptr->type, ptr->id,
+					ptr->err, ptr->data0, ptr->data1);
+			list_move_tail(&ptr->list,
+					&vspadev->events_queued_list);
+			last = ptr;
+			vspadev->event_list_mask |= 0x10 << er->type;
+		}
+		eq->idx_dequeue = event_next_index(eq->idx_dequeue);
+	}
+
+	/* update event list mask if needed */
+	if (mask == 0) {
+		list_for_each_entry(ptr, &vspadev->events_queued_list, list) {
+			mask |= ptr->type;
+		}
+		vspadev->event_list_mask = mask & VSPA_MSG_ALL_EVENTS;
+	}
+
+	spin_unlock(&vspadev->event_list_lock);
+	/* TODO - report lost events */
+}
+
+static const int event_size[16] = {
+	0, 0, 0, 0, 0, 8, 4, 0, 0, 8, 0, 0, 0, 0, 0, 0
+};
+
+int read_event(struct vspa_device *vspadev,
+	struct vspa_event_read *evt_rd)
+{
+	struct event_list *ptr, *ptr1;
+	uint32_t buf[sizeof(struct vspa_event)/sizeof(uint32_t) + 2];
+	struct vspa_event *evt = (struct vspa_event *)buf;
+	uint32_t *payload = &evt->data[0];
+	int err;
+	unsigned int mask;
+	int type;
+	uint32_t *src_ptr;
+	uint32_t src_len;
+	size_t length;
+	struct list_head *prev;
+	int timeout;
+	unsigned long start_time, new_time;
+	size_t len;
+	int reply_bd_idx, start;
+
+	if (vspadev->state == VSPA_STATE_UNKNOWN)
+		return -ENODATA;
+
+	start_time = jiffies;
+
+	event_list_update(vspadev);
+
+	/* Convert timeout to jiffies */
+	timeout = evt_rd->timeout;
+	if (timeout > 0)
+		timeout = msecs_to_jiffies(timeout);
+	/* If no filter is specified then match all message types */
+	mask = evt_rd->event_mask & VSPA_MSG_ALL;
+	if ((mask & VSPA_MSG_ALL_EVENTS) == 0)
+		mask |= VSPA_MSG_ALL_EVENTS;
+
+	spin_lock(&vspadev->event_list_lock);
+	while (((vspadev->event_list_mask & mask) == 0) &&
+		(vspadev->state > VSPA_STATE_POWER_DOWN)) {
+		/* nothing to read */
+		spin_unlock(&vspadev->event_list_lock);
+		if (vspadev->state <= VSPA_STATE_POWER_DOWN)
+			return -ENODATA;
+		if (timeout == 0) /* non-blocking */
+			return -EAGAIN;
+		pr_debug("vspa%d sleep: mask %04X, elm %04X, ep %04X, st %d\n",
+				vspadev->id, mask, vspadev->event_list_mask,
+				events_present(vspadev), vspadev->state);
+		if (timeout < 0) {
+			err = wait_event_interruptible(vspadev->event_wait_q,
+				((events_present(vspadev) & mask) ||
+				 (vspadev->state <= VSPA_STATE_POWER_DOWN)));
+			if (err < 0)
+				return err;
+		} else {
+			err = wait_event_interruptible_timeout(
+				vspadev->event_wait_q,
+				((events_present(vspadev) & mask) ||
+				 (vspadev->state <= VSPA_STATE_POWER_DOWN)),
+				timeout);
+			if (err < 0)
+				return err;
+			new_time = jiffies;
+			timeout -= new_time - start_time;
+			start_time = new_time;
+			if (timeout < 0)
+				timeout = 0;
+		}
+
+		pr_debug("vspa%d wakup: mask %04X, elm %04X, ep %04X, st %d\n",
+				vspadev->id, mask, vspadev->event_list_mask,
+				events_present(vspadev), vspadev->state);
+
+		event_list_update(vspadev);
+		/* otherwise loop, but first reacquire the lock */
+		spin_lock(&vspadev->event_list_lock);
+	}
+
+	/* Find first event that matches the mask */
+	type = 0;
+	list_for_each_entry(ptr, &vspadev->events_queued_list, list) {
+		pr_debug("vspa%d: read %04X check %04X\n", vspadev->id, mask,
+				ptr->type);
+		if (ptr->type & mask) {
+			type = ptr->type & 0xF;
+			break;
+		}
+	}
+	if (!type) {
+		spin_unlock(&vspadev->event_list_lock);
+		return (vspadev->state <= VSPA_STATE_POWER_DOWN) ? -ENODATA :
+								   -EAGAIN;
+	}
+	src_len = 0;
+	src_ptr = NULL;
+	reply_bd_idx = -1;
+	if (type == VSPA_EVENT_REPLY) {
+		reply_bd_idx = ptr->data1 >> 24;
+		if (reply_bd_idx >= BD_ENTRIES)
+			reply_bd_idx = -1;
+		else {
+			start   = vspadev->reply_pool.bd[reply_bd_idx].wstart;
+			if (start >= 0) {
+				src_len = ptr->data1 & 0xFFFF;
+				src_ptr = &vspadev->reply_pool.vaddr[start];
+				IF_DEBUG(DEBUG_REPLY) {
+					int i;
+					uint32_t *st = src_ptr;
+
+					pr_info("vspa%d: evt_read Reply %08X %d->%04X %d\n",
+					vspadev->id, ptr->data1, reply_bd_idx,
+					start, src_len);
+					for (i = src_len/4; i > 0;  i--)
+						pr_info("%02d = %08X\n", i, *st++);
+				}
+			}
+		}
+	} else if (type == VSPA_EVENT_SPM) {
+		start   = ptr->data1 >> 16;
+		src_len = ptr->data1 & 0xFFFF;
+		src_ptr = &vspadev->spm_buf_vaddr[start];
+		IF_DEBUG(DEBUG_SPM) {
+			int i;
+			uint32_t *st = src_ptr;
+
+			pr_info("vspa%d: evt_read SPM %08X %04X %d\n",
+				vspadev->id, ptr->data1, start, src_len);
+			for (i = src_len/4; i > 0; i--)
+				pr_info("[%2d] %08X\n", i, *st++);
+		}
+	}
+
+	length = event_size[type];
+	evt->control  = ptr->control;
+	evt->type     = type;
+	evt->pkt_size = length ? length : src_len;
+	payload[0]    = ptr->data0;
+	payload[1]    = ptr->data1;
+
+	src_len = evt_rd->buf_len - sizeof(*evt);
+	if (src_len > evt->pkt_size)
+		src_len = evt->pkt_size;
+	if (evt_rd->buf_len <  sizeof(*evt))
+		src_len = 0;
+	evt->buf_size = src_len;
+
+	IF_DEBUG(DEBUG_EVENT) {
+		pr_info("vspa%d: evt_read %2d %2d %2d %2d %08X %08X\n",
+			vspadev->id, type, evt->err, evt->pkt_size,
+			 evt->buf_size, ptr->data0, ptr->data1);
+	}
+
+	/* free the message unless PEEKing */
+	if (!(mask & VSPA_MSG_PEEK)) {
+		prev = ptr->list.prev;
+		list_move_tail(&ptr->list, &vspadev->events_free_list);
+		/* If not at the end of the list try coalescing messages */
+		if (prev->next != &vspadev->events_queued_list &&
+			    prev != &vspadev->events_queued_list) {
+			/* try to coalese Watchdog errors */
+			ptr = list_entry(prev, struct event_list, list);
+			ptr1 = list_first_entry(prev, struct event_list, list);
+			if (ptr->type ==
+				((0x10<<VSPA_EVENT_ERROR)|VSPA_EVENT_ERROR) &&
+			    ptr1->type ==
+				((0x10<<VSPA_EVENT_ERROR)|VSPA_EVENT_ERROR) &&
+			    ptr->id == ptr1->id &&
+			    ptr->id == VSPA_ERR_WATCHDOG) {
+				ptr->data0 = ptr1->data0;
+				ptr->data1++;
+				list_move_tail(&ptr1->list,
+					&vspadev->events_free_list);
+			}
+		}
+		/* update event list mask */
+		mask = 0;
+		list_for_each_entry(ptr, &vspadev->events_queued_list, list) {
+			mask |= ptr->type;
+		}
+		vspadev->event_list_mask = mask & VSPA_MSG_ALL_EVENTS;
+	}
+
+	/* only copy up to the length of the message */
+	length += sizeof(*evt);
+	len = evt_rd->buf_len;
+	if (length > len)
+		length = len;
+	err = copy_to_user(evt_rd->buf_ptr, evt, length);
+
+	/* copy the data buffer contents if needed */
+	len -= length;
+	if (err == 0 && src_ptr && src_len > 0 && len > 0) {
+		if (src_len > len)
+			src_len = len;
+		length += src_len;
+		err = copy_to_user(&(evt_rd->buf_ptr->data[0]),
+				src_ptr, src_len);
+	}
+
+	/* Release reply buffer */
+	if ((reply_bd_idx >= 0) && (!(mask & VSPA_MSG_PEEK)))
+		pool_free_bd(&vspadev->reply_pool, reply_bd_idx);
+
+	spin_unlock(&vspadev->event_list_lock);
+	pr_debug("vspa%d: err %d, length %zu bytes\n", vspadev->id, err,
+			length);
+	return err ? -EFAULT : length;
+}
diff --git a/drivers/misc/vspa/main.c b/drivers/misc/vspa/main.c
new file mode 100644
index 0000000..443e596
--- /dev/null
+++ b/drivers/misc/vspa/main.c
@@ -0,0 +1,2510 @@
+/*
+ * drivers/misc/vspa/main.c
+ * VSPA device driver
+ *
+ * Copyright (C) 2016 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/types.h>
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/list.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/wait.h>
+#include <linux/uaccess.h>
+#include <linux/signal.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/input.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_gpio.h>
+#include <linux/of_address.h>
+#include <linux/pid.h>
+#include <linux/mm.h>
+#include <linux/poll.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <linux/list.h>
+
+#include <uapi/linux/vspa.h>
+#include "vspa.h"
+
+/*
+ * TODO - Add CMD_ERR event
+ * TODO - SPM error handing
+ */
+
+/* Additional options for checking if Mailbox write to VSP completed */
+#ifdef VSPA_DEBUG
+#define MB_CHECK_ON_WRITE	/* Check when queuing a new mailbox write */
+#define MB_CHECK_IN_IRQ	/* Check during Mailbox IRQ processing    */
+#define MB_CHECK_TIMER	/* Use timer to keep checking after MB OUT*/
+#endif
+
+static int cmd_buffer_bytes = 1024;
+module_param(cmd_buffer_bytes, int, 0644);
+MODULE_PARM_DESC(cmd_buffer_bytes, "Size of cmdbuf(bytes), default: 1024");
+
+static int reply_buffer_bytes = 1024;
+module_param(reply_buffer_bytes, int, 0644);
+MODULE_PARM_DESC(reply_buffer_bytes, "Size of replybuf(bytes), default: 1024");
+
+static int spm_buffer_bytes = 4096;
+module_param(spm_buffer_bytes, int, 0644);
+MODULE_PARM_DESC(spm_buffer_bytes, "Size of spm buf(bytes), default: 4096");
+
+/* Number of VSPA devices probed on system */
+static int vspa_devs;
+static s32 vspa_major;
+static s32 vspa_minor;
+static struct class *vspa_class;
+
+#define VSPA_HALT_TIMEOUT (100000)
+#define VSPA_STARTUP_TIMEOUT (100000)
+
+#define CONTROL_REG_MASK	(~0x000100FF)
+#define CONTROL_PDN_EN		(1<<31)
+#define CONTROL_HOST_MSG_GO	(1<<20 | 1<<21 | 1<<22 | 1<<23)
+#define CONTROL_VCPU_RESET	(1<<16)
+#define CONTROL_DEBUG_MSG_GO	(1<<5)
+#define CONTROL_IPPU_GO		(1<<1)
+#define CONTROL_HOST_GO		(1<<0)
+
+#define MAX_STRING_ATU_WIN 256
+#define VSPA_ATU_MIN_WIN_SZ 16
+#define SZ_TO_MASK(sz) ((1 << sz) - 1)
+#define ATU_WINDW_BASE_ADD(win_num) \
+	vspadev->vspa_atu_win[win_num].windw_base_add
+#define ATU_WINDW_SZ(win_num) \
+	vspadev->vspa_atu_win[win_num].windw_sz
+
+
+/*********************** DMA Request Queue ****************************/
+
+static int dma_next_index(int curr)
+{
+	return (curr == (DMA_QUEUE_ENTRIES - 1)) ? 0 : curr+1;
+}
+
+static void dma_reset_queue(struct vspa_device *vspadev)
+{
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&vspadev->dma_tx_queue_lock, irqflags);
+	vspadev->dma_queue.idx_dma = 0;
+	spin_unlock_irqrestore(&vspadev->dma_tx_queue_lock, irqflags);
+	spin_lock(&vspadev->dma_enqueue_lock);
+	vspadev->dma_queue.idx_queue = 0;
+	vspadev->dma_queue.idx_chk = 0;
+	vspadev->dma_queue.chan = 0;
+	vspadev->cmd_dma_chan   = 0;
+	spin_unlock(&vspadev->dma_enqueue_lock);
+}
+
+u32 vspa_map_axi_addr_atu_win(struct vspa_device *vspadev,
+		dma_addr_t dma_axi_addr)
+{
+	int atu_win_cnt;
+
+	/* Find the windows with which this address belongs to */
+	for (atu_win_cnt = 0; atu_win_cnt < vspadev->num_atu_win;
+			atu_win_cnt++) {
+		if (((u32)dma_axi_addr &
+				(~SZ_TO_MASK(ATU_WINDW_SZ(atu_win_cnt)))) ==
+				(u32)ATU_WINDW_BASE_ADD(atu_win_cnt)) {
+			/* Window Found
+			 * Map the address to window and return */
+			return ((u32)(u64)ATU_WINDW_BASE_ADD(atu_win_cnt) +
+					(dma_axi_addr &
+					 SZ_TO_MASK(ATU_WINDW_SZ(atu_win_cnt))));
+		}
+	}
+
+	return -1U;
+}
+
+/* sometimes called from IRQ handler */
+static int dma_transmit(struct vspa_device *vspadev)
+{
+	int ret = 0;
+	u32 __iomem *regs = vspadev->regs;
+	struct dma_queue *dq = &(vspadev->dma_queue);
+	struct vspa_dma_req *dr;
+	unsigned long irqflags;
+	u32 mapped_axi_addr;
+
+	spin_lock_irqsave(&vspadev->dma_tx_queue_lock, irqflags);
+	dr = &(dq->entry[dq->idx_dma]);
+	if (dq->idx_dma == dq->idx_queue) /* Queue is empty */
+		ret = -ENOMSG;
+	else if (dq->idx_dma != dq->idx_chk) /* DMA is in process */
+		ret = -EBUSY;
+	else {
+		/* Map the DMA AXI address to ATU window */
+		mapped_axi_addr = vspa_map_axi_addr_atu_win(vspadev, dr->axi_addr);
+		if (-1U == mapped_axi_addr) {
+			ret = -EINVAL;
+			goto hndl_retval;
+		}
+		/* Program the DMA transfer */
+		vspa_reg_write(regs + DMA_DMEM_ADDR_REG_OFFSET, dr->dmem_addr);
+		vspa_reg_write(regs + DMA_AXI_ADDR_REG_OFFSET,
+				mapped_axi_addr);
+		vspa_reg_write(regs + DMA_BYTE_CNT_REG_OFFSET,  dr->byte_cnt);
+		vspa_reg_write(regs + DMA_XFR_CTRL_REG_OFFSET,  dr->xfr_ctrl);
+		/* Update the queue */
+		dq->idx_dma = dma_next_index(dq->idx_dma);
+	}
+
+hndl_retval:
+	spin_unlock_irqrestore(&vspadev->dma_tx_queue_lock, irqflags);
+	return ret;
+}
+
+static int dma_enqueue(struct vspa_device *vspadev, struct vspa_dma_req *req)
+{
+	int ret = 0;
+	struct dma_queue *dq = &(vspadev->dma_queue);
+	struct vspa_dma_req *dr;
+	int next_slot;
+
+	spin_lock(&vspadev->dma_enqueue_lock);
+
+	dr = &(dq->entry[dq->idx_queue]);
+	next_slot = dma_next_index(dq->idx_queue);
+
+	if (next_slot == dq->idx_chk) /* Queue is full */
+		ret = -ENOMEM;
+	else {
+		pr_debug("DMA Enqueue(%d)\n", dq->idx_queue);
+		dr->control   = req->control;
+		dr->dmem_addr = req->dmem_addr;
+		dr->axi_addr  = req->axi_addr;
+		dr->byte_cnt  = req->byte_cnt;
+		dr->xfr_ctrl  = req->xfr_ctrl | 0x4000; /* IRQ_EN */
+		dq->chan      = dr->xfr_ctrl & 0x1F;
+
+		IF_DEBUG(DEBUG_DMA)
+			pr_info("vspa%d dma: %08X %08X %016llX %08X %08X\n",
+				vspadev->id, dr->control, dr->dmem_addr,
+				dr->axi_addr, dr->byte_cnt, dr->xfr_ctrl);
+		/* barrier */
+		dq->idx_queue = next_slot;
+	}
+	spin_unlock(&vspadev->dma_enqueue_lock);
+	dma_transmit(vspadev);
+	return ret;
+}
+
+/************  Command / Reply Memory Pool Management **************/
+static void cbuffer_reset(struct circular_buffer *cbuf)
+{
+	cbuf->write_idx = 0;
+	cbuf->read_idx = 0;
+}
+
+static void cbuffer_init(struct circular_buffer *cbuf, uint32_t size,
+	uint32_t *paddr, uint32_t *vaddr)
+{
+	cbuf->size = size;
+	cbuf->paddr = paddr;
+	cbuf->vaddr = vaddr;
+	cbuffer_reset(cbuf);
+}
+
+/* Make sure size is AXI aligned */
+static int cbuffer_add(struct circular_buffer *cbuf, uint32_t size)
+{
+	uint32_t space;
+	uint32_t index;
+	uint32_t new_idx;
+
+	if (cbuf->write_idx >= cbuf->read_idx) {
+		index = cbuf->write_idx;
+		space = cbuf->size - index;
+		if (space < size) {
+			index = 0;
+			space = cbuf->read_idx;
+		}
+	} else {
+		index = cbuf->write_idx;
+		space = cbuf->read_idx - index;
+	}
+
+	if (space < size) {
+		/* as a last resort resort reset the indexes if empty */
+		if (cbuf->write_idx == cbuf->read_idx && cbuf->size >= size) {
+			cbuf->write_idx = 0;
+			cbuf->read_idx = 0;
+			index = 0;
+		} else
+			return -ENOBUFS;
+	}
+
+	new_idx = index + size;
+	if (new_idx == cbuf->size)
+		new_idx = 0;
+	if (new_idx == cbuf->read_idx)
+		return -ENOBUFS;
+
+	cbuf->write_idx = new_idx;
+	return index;
+}
+
+static void cbuffer_free(struct circular_buffer *cbuf,
+		uint32_t index, uint32_t size)
+{
+	cbuf->read_idx = index + size;
+	if (cbuf->read_idx >= cbuf->size)
+		cbuf->read_idx = 0;
+}
+
+static void pool_print(struct memory_pool *pool)
+{
+	int i;
+
+	pr_info("Pool: free = %04X, tail = %d\n", pool->free_bds, pool->tail);
+	for (i = 0; i < BD_ENTRIES; i++) {
+		pr_info("[%02d] = %4d %4d %3d %3d %2d\n", i,
+			pool->bd[i].start, pool->bd[i].wstart,
+			pool->bd[i].size, pool->bd[i].free,
+			pool->bd[i].next);
+	}
+}
+
+/* Bit map manipulation for checking a free command descriptor */
+#define GET_FREE_BD(n) { \
+	n = ffs(pool->free_bds); \
+	if ((n > BD_ENTRIES) || (n == 0)) \
+		n = -1; \
+	else {\
+		n = n - 1;\
+		pool->free_bds &= ~(1UL << n);\
+		} \
+	}
+
+static void pool_reset(struct memory_pool *pool)
+{
+	int i;
+
+	for (i = 0; i < BD_ENTRIES; i++) {
+		pool->bd[i].wstart = -1;
+		pool->bd[i].start = 0;
+		pool->bd[i].size = 0;
+		pool->bd[i].free = 0;
+		pool->bd[i].next = -1;
+	}
+	pool->free_bds = (1 << BD_ENTRIES) - 1;
+	GET_FREE_BD(i);
+	if (i < 0)
+		return;
+	pool->bd[i].start = pool->size;
+	pool->bd[i].free = pool->size;
+	pool->bd[i].next = i;
+	pool->tail = i;
+}
+
+static void pool_init(struct memory_pool *pool, uint32_t size,
+	uint32_t *paddr, uint32_t *vaddr)
+{
+	pool->free_bds = 0;
+	pool->size = size;
+	pool->paddr = paddr;
+	pool->vaddr = vaddr;
+	pool_reset(pool);
+}
+
+/**
+ * @brief : This gets the free command descriptor
+ * for placing the command of requested size
+ * @return : valid BD index else negative
+ */
+
+static int8_t pool_get_bd(struct memory_pool *pool, uint32_t size)
+{
+	int i = -1, j = -1, k = -1;
+
+	if (size == 0)
+		return -EINVAL;
+
+	pr_debug("Pool_get_bd: size = %d\n", size);
+	/* Find next buffer descriptor that has enough free space */
+	for (i = pool->tail; pool->bd[i].free < size; i = pool->bd[i].next) {
+		/* Did we wrap around and no large block found? */
+		if (pool->bd[i].next == pool->tail)
+			return -ENOSPC;
+	}
+
+	/* We found a bigger block ? */
+	if (pool->bd[i].free > size) {
+		/* We need to break it up*/
+		GET_FREE_BD(j);
+		if (j == -1)
+			return -ENOSPC;
+		pool->tail = j;
+		k = pool->bd[i].next;
+		pool->bd[j].next = k;
+		pool->bd[i].next = j;
+		pool->bd[j].free = pool->bd[i].free - size;
+		pool->bd[j].size = 0;
+		pool->bd[j].start = pool->bd[i].start - size;
+	} else {
+		pool->tail = i;
+	}
+	pool->bd[i].free = 0;
+	pool->bd[i].size = size;
+	pool->bd[i].wstart = pool->bd[i].start - pool->bd[i].size;
+	return i;
+}
+
+void pool_free_bd(struct memory_pool *pool, uint8_t index)
+{
+	uint32_t size;
+
+	pr_debug("pool_free: %d\n", index);
+	if (index >= BD_ENTRIES)
+		return;
+
+	pool->bd[index].wstart = -1;
+	size = pool->bd[index].size;
+	pool->bd[index].size = 0;
+	pool->bd[index].free = size;
+}
+
+static void pool_consolidate(struct memory_pool *pool, uint32_t wstart)
+{
+	int index, i;
+	int cons = 0;
+
+	for (index = 0; pool->bd[index].next > 0; index = i) {
+		i = pool->bd[index].next;
+		/* This and next BD must both have free space */
+		if (pool->bd[index].free == 0 || pool->bd[i].free == 0)
+			continue;
+		/* Don't collapse BD at wstart */
+		if (pool->bd[i].start == wstart)
+			continue;
+
+		pool->bd[index].free += pool->bd[i].free;
+		pool->bd[index].next = pool->bd[i].next;
+		pool->bd[i].wstart = -1;
+		pool->bd[i].start = 0;
+		pool->bd[i].size = 0;
+		pool->bd[i].free = 0;
+		pool->bd[i].next = -1;
+		if (pool->tail == i)
+			pool->tail = index;
+		pool->free_bds |= (1UL << i);
+		i = pool->bd[index].next;
+		cons++;
+	}
+}
+
+/******************** Sequence ID Management ***********************/
+
+static void seqid_reset(struct vspa_device *vspadev)
+{
+	int seqid;
+
+	vspadev->active_seqids = 0;
+	vspadev->last_seqid = 0;
+	for (seqid = 0; seqid < MAX_SEQIDS; seqid++)
+		vspadev->seqid[seqid].cmd_id = -1;
+}
+
+static int seqid_get_next(struct vspa_device *vspadev)
+{
+	int next;
+	uint32_t ids = (vspadev->active_seqids) << MAX_SEQIDS |
+				vspadev->active_seqids;
+
+	ids = ids >> (vspadev->last_seqid + 1);
+	next = ffz(ids);
+	if (next > MAX_SEQIDS)
+		return -ENOSR;
+	next += (vspadev->last_seqid + 1);
+	next &= MAX_SEQIDS - 1;
+
+	IF_DEBUG(DEBUG_SEQID)
+		pr_info("seqid: active %04X, last %d, ids %08X, next %d\n",
+		vspadev->active_seqids, vspadev->last_seqid, ids, next);
+
+	vspadev->active_seqids |= 1 << next;
+	vspadev->last_seqid = next;
+	vspadev->seqid[next].cmd_id = -1;
+	return next;
+}
+
+static void seqid_release(struct vspa_device *vspadev, int seqid)
+{
+	IF_DEBUG(DEBUG_SEQID)
+		pr_info("seqid_release(%d)\n", seqid);
+
+	if (seqid < 0 || seqid >= MAX_SEQIDS)
+		return;
+
+	if (vspadev->seqid[seqid].cmd_id >= 0) {
+		vspadev->seqid[seqid].cmd_id = -1;
+
+		if (vspadev->seqid[seqid].cmd_buffer_idx >= 0) {
+			cbuffer_free(&vspadev->cmd_buffer,
+				vspadev->seqid[seqid].cmd_buffer_idx,
+				vspadev->seqid[seqid].cmd_buffer_size);
+			vspadev->seqid[seqid].cmd_buffer_idx = -1;
+		}
+
+		if (vspadev->seqid[seqid].cmd_bd_index >= 0) {
+			pool_free_bd(&vspadev->cmd_pool,
+				vspadev->seqid[seqid].cmd_bd_index);
+			vspadev->seqid[seqid].cmd_bd_index = -1;
+		}
+
+		if (vspadev->seqid[seqid].reply_bd_index >= 0) {
+			pool_free_bd(&vspadev->reply_pool,
+				vspadev->seqid[seqid].reply_bd_index);
+			vspadev->seqid[seqid].reply_bd_index = -1;
+		}
+	}
+	vspadev->active_seqids &= ~(1 << seqid);
+}
+
+/************************ Command Buffer ***************************/
+
+static void cmd_reset(struct vspa_device *vspadev)
+{
+	vspadev->first_cmd = 1;
+	vspadev->cmd_pool.size = 0;
+	cbuffer_reset(&vspadev->cmd_buffer);
+	pool_reset(&vspadev->cmd_pool);
+	pool_reset(&vspadev->reply_pool);
+	seqid_reset(vspadev);
+	/* clear SPM buffer */
+	memset(vspadev->spm_buf_vaddr, 0, vspadev->spm_buf_bytes);
+	vspadev->spm_addr = (vspadev->spm_buf_bytes - 8) >> 3;
+}
+
+/************************ SPM Processing ***************************/
+
+static void spm_update(struct vspa_device *vspadev, uint32_t flags)
+{
+	dma_addr_t ptr;
+	int size;
+	int size_max;
+	int err;
+
+	if (!vspadev->spm_buf_vaddr) {
+		pr_err("vspa%d: vspadev->spm_buf_vaddr not initialized.\n",
+				vspadev->id);
+		return;
+	}
+
+	err = 0; /* TODO flags ?? */
+	if (flags & DMA_FLAG_CFGERR)
+		err = VSPA_ERR_DMA_CFGERR;
+	else
+		if (flags & DMA_FLAG_XFRERR)
+			err = VSPA_ERR_DMA_XFRERR;
+	/* TODO handle error case */
+
+	ptr = vspadev->spm_buf_vaddr[vspadev->spm_addr];
+	IF_DEBUG(DEBUG_SPM) {
+		pr_info("vspa%d: SPM DMA IRQ, %d\n", vspadev->id, flags);
+		pr_info("[%04X] = %016llX\n", vspadev->spm_addr, ptr);
+		IF_DEBUG(DEBUG_TEST_SPM) {
+			int i;
+
+			for (i = 0; i < (vspadev->spm_buf_bytes>>2); i++) {
+				if (vspadev->spm_buf_vaddr[i])
+					pr_info("[%04X] = %08X\n", i,
+						vspadev->spm_buf_vaddr[i]);
+			}
+		}
+	}
+
+	while (ptr) {
+		if (ptr >= (dma_addr_t)vspadev->spm_buf_paddr)
+			ptr -= (dma_addr_t)vspadev->spm_buf_paddr;
+		if (ptr >= vspadev->spm_buf_bytes || ptr & 3) {
+			vspadev->spm_buf_vaddr[vspadev->spm_addr] = 0;
+			ERR("%d: bad SPM ptr %016llX\n", vspadev->id, ptr);
+			/* TODO report */
+		} else {
+			ptr >>= 3;
+			size_max = ptr > vspadev->spm_addr ?
+					(vspadev->spm_buf_bytes >> 3) - ptr :
+					vspadev->spm_addr - ptr;
+			size = (vspadev->spm_buf_vaddr[ptr] >> 16) & 0xFF;
+			if (size >= size_max)
+				size = size_max - 1;
+			err = 0;
+			event_enqueue(vspadev, VSPA_EVENT_SPM, 0, err,
+				      vspadev->spm_buf_vaddr[ptr],
+				      (ptr << 16) | (size << 2));
+			vspadev->spm_addr = ptr - 1;
+			IF_DEBUG(DEBUG_SPM) {
+				while (size >= 0) {
+					pr_info("[%04llX] = %08X\n", ptr,
+						vspadev->spm_buf_vaddr[ptr]);
+					ptr++;
+					size--;
+				}
+			}
+		}
+		ptr = vspadev->spm_buf_vaddr[vspadev->spm_addr];
+		IF_DEBUG(DEBUG_SPM) {
+			pr_info("vspa%d: SPM cont\n", vspadev->id);
+			pr_info("[%04X] = %016llX\n", vspadev->spm_addr, ptr);
+		}
+	}
+}
+
+/************************ Mailbox Queues ***************************/
+
+static void mbox_reset(struct vspa_device *vspadev)
+{
+	vspadev->mb64_queue.idx_enqueue = 0;
+	vspadev->mb64_queue.idx_dequeue = 0;
+	vspadev->mb64_queue.idx_complete = 0;
+}
+
+static int mbox_next_slot(struct mbox_queue *queue)
+{
+	int next_slot = queue->idx_enqueue + 1;
+
+	if (next_slot == MBOX_QUEUE_ENTRIES)
+		next_slot = 0;
+
+	if (next_slot == queue->idx_dequeue) /* Queue is full */
+		return -ENOMEM;
+
+	return next_slot;
+}
+
+static int mb64_transmit(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	struct mbox_queue *mq = &(vspadev->mb64_queue);
+	unsigned long irqflags;
+	struct vspa_mb64 *me;
+	int next_slot;
+	int ret = 0;
+
+	spin_lock_irqsave(&vspadev->mb64_lock, irqflags);
+	if (mq->idx_dequeue == mq->idx_enqueue) /* Queue is empty */
+		ret = -ENOMSG;
+	else if (mq->idx_dequeue != mq->idx_complete) /* MBOX is in process */
+		ret = -EBUSY;
+	else {
+		me = &(vspadev->mb64[mq->idx_dequeue]);
+		vspa_reg_write(regs + HOST_OUT_1_64_MSB_REG_OFFSET,
+				me->data_msb);
+		vspa_reg_write(regs + HOST_OUT_1_64_LSB_REG_OFFSET,
+				me->data_lsb);
+		IF_DEBUG(DEBUG_MBOX64_OUT)
+			pr_info("vspa%d: wrote 0x%08X 0x%08X to mbox64\n",
+				vspadev->id, me->data_msb, me->data_lsb);
+		next_slot = mq->idx_dequeue + 1;
+		if (next_slot == MBOX_QUEUE_ENTRIES)
+			next_slot = 0;
+		mq->idx_dequeue = next_slot;
+	}
+	spin_unlock_irqrestore(&vspadev->mb64_lock, irqflags);
+	return ret;
+}
+
+static int mb_transmit_check(struct vspa_device *vspadev)
+{
+	int idx;
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t status = vspa_reg_read(regs + HOST_MBOX_STATUS_REG_OFFSET);
+	unsigned long irqflags;
+	int requeue = 0;
+
+	spin_lock_irqsave(&vspadev->mbchk_lock, irqflags);
+	if (vspadev->mb64_queue.idx_dequeue !=
+	    vspadev->mb64_queue.idx_complete) {	/* MBOX is in process */
+		requeue = 1;
+		if (!(status & MBOX_STATUS_OUT_1_64_BIT)) {
+			idx = vspadev->mb64_queue.idx_dequeue;
+			IF_DEBUG(DEBUG_MBOX64_OUT)
+				pr_info("vspa%d: mbox64 out idx %d consumed\n",
+					vspadev->id, idx);
+			if (vspadev->mb64[idx].flags &
+						VSPA_FLAG_REPORT_MB_COMPLETE)
+				event_enqueue(vspadev, VSPA_EVENT_MB64_OUT,
+						vspadev->mb64[idx].id, 0,
+						vspadev->mb64[idx].data_msb,
+						vspadev->mb64[idx].data_lsb);
+			vspadev->mb64_queue.idx_complete = idx;
+			mb64_transmit(vspadev);
+		}
+	}
+
+	spin_unlock_irqrestore(&vspadev->mbchk_lock, irqflags);
+	return requeue;
+}
+
+static void mbchk_callback(unsigned long data)
+{
+	struct vspa_device *vspadev = (struct vspa_device *)data;
+
+#ifdef MB_CHECK_TIMER
+	if (mb_transmit_check(vspadev))
+		mod_timer(&vspadev->mbchk_timer, jiffies + 1);
+	else
+#endif
+		complete(&vspadev->mbchk_complete);
+}
+
+/*************************** Watchdog ******************************/
+
+static void watchdog_callback(unsigned long data)
+{
+	struct vspa_device *vspadev = (struct vspa_device *)data;
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t val;
+
+	if (vspadev->state == VSPA_STATE_RUNNING_IDLE &&
+	    vspadev->watchdog_interval_msecs > 0) {
+		val = vspa_reg_read(regs + GP_OUT3_REG_OFFSET) >> 16;
+		IF_DEBUG(DEBUG_WATCHDOG)
+			pr_info("vspa%d: watchdog_value = %04X\n",
+							vspadev->id, val);
+		if (val == vspadev->watchdog_value) {
+			event_enqueue(vspadev, VSPA_EVENT_ERROR,
+					VSPA_ERR_WATCHDOG, 0, val, 0);
+		}
+		vspadev->watchdog_value = val;
+		val = vspa_reg_read(regs + CONTROL_REG_OFFSET);
+		val = (val & CONTROL_REG_MASK) | CONTROL_HOST_GO;
+		vspa_reg_write(regs + CONTROL_REG_OFFSET, val);
+		mod_timer(&vspadev->watchdog_timer, jiffies +
+			msecs_to_jiffies(vspadev->watchdog_interval_msecs));
+	} else {
+		complete(&vspadev->watchdog_complete);
+	}
+
+	event_list_update(vspadev);
+}
+
+/**************************** IRQ  *********************************/
+
+static void vspa_enable_dma_irqs(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t irqen  = vspa_reg_read(regs + IRQEN_REG_OFFSET);
+
+	vspa_reg_write(regs + DMA_IRQ_STAT_REG_OFFSET, 0xFFFFFFFFUL);
+	vspa_reg_write(regs + DMA_COMP_STAT_REG_OFFSET, 0xFFFFFFFFUL);
+	vspa_reg_write(regs + DMA_XFRERR_STAT_REG_OFFSET, 0xFFFFFFFFUL);
+	vspa_reg_write(regs + DMA_CFGERR_STAT_REG_OFFSET, 0xFFFFFFFFUL);
+
+	/* Enable DMA complete and DMA error IRQs */
+	irqen |= 0x30;
+	vspa_reg_write(regs + IRQEN_REG_OFFSET, irqen);
+}
+
+static void vspa_enable_mailbox_irqs(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t irqen  = vspa_reg_read(regs + IRQEN_REG_OFFSET);
+
+	vspa_reg_write(regs + HOST_FLAGS0_REG_OFFSET, 0xFFFFFFFFUL);
+	vspa_reg_write(regs + HOST_FLAGS1_REG_OFFSET, 0xFFFFFFFFUL);
+
+	vspa_reg_read(regs + HOST_IN_64_MSB_REG_OFFSET);
+	vspa_reg_read(regs + HOST_IN_64_LSB_REG_OFFSET);
+	vspa_reg_write(regs + STATUS_REG_OFFSET, STATUS_REG_IRQ_VCPU_SENT_MSG);
+
+	/*Enable Mailbox anf Flag IRQs */
+	irqen |= 0xF00C;
+	vspa_reg_write(regs + IRQEN_REG_OFFSET, irqen);
+}
+
+/* Command Reply has been sent */
+static void vspa_flags1_irq_handler(struct vspa_device *vspadev)
+{
+	int seqid;
+	uint32_t clr_flags0 = 0;
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t flags1 = vspa_reg_read(regs + HOST_FLAGS1_REG_OFFSET);
+	uint32_t flags0 = vspa_reg_read(regs + HOST_FLAGS0_REG_OFFSET);
+
+	vspa_reg_write(regs + HOST_FLAGS1_REG_OFFSET, flags1);
+
+	/* spin_lock(&vspadev->irq_lock); */
+	if (vspadev->irq_bits) {
+		pr_err("vspa%d flg1 irqs = %d\n",
+		vspadev->id, vspadev->irq_bits);
+	}
+	vspadev->irq_bits |= 1;
+
+	IF_DEBUG(DEBUG_FLAGS1_IRQ)
+		pr_info("vspa%d: flags1 = %08x, flags0 = %08x\n",
+				 vspadev->id, flags1, flags0);
+
+	while (flags1) {
+		seqid = ffs(flags1) - 1;
+		pr_debug("flags1 = %08X, flags0 = %08X, seqid = %d\n", flags1,
+				flags0, seqid);
+		flags1 &= ~(1 << seqid);
+		if (vspadev->seqid[seqid].cmd_id < 0) {
+			ERR("%d: unmatched seqid %d - flag ignored\n",
+				vspadev->id, seqid);
+			if (flags0 & (1 << seqid))
+				clr_flags0 |= 1 << seqid;
+			/* TODO - report error event */
+			continue;
+		}
+		if (flags0 & (1 << seqid)) {
+			clr_flags0 |= 1 << seqid;
+			if (vspadev->seqid[seqid].flags &
+						VSPA_FLAG_REPORT_CMD_CONSUMED)
+				event_enqueue(vspadev, VSPA_EVENT_CMD,
+				      vspadev->seqid[seqid].cmd_id, 0,
+				      vspadev->seqid[seqid].payload1,
+				      vspadev->seqid[seqid].reply_size);
+		}
+		if (vspadev->seqid[seqid].flags & VSPA_FLAG_REPORT_CMD_REPLY)
+			event_enqueue(vspadev, VSPA_EVENT_REPLY,
+			      vspadev->seqid[seqid].cmd_id, 0,
+			      vspadev->seqid[seqid].payload1,
+			      vspadev->seqid[seqid].reply_size |
+			      vspadev->seqid[seqid].reply_bd_index << 24);
+		vspadev->seqid[seqid].reply_bd_index = -1;
+		seqid_release(vspadev, seqid);
+	}
+	if (clr_flags0)
+		vspa_reg_write(regs + HOST_FLAGS0_REG_OFFSET, clr_flags0);
+
+	/* spin_unlock(&vspadev->irq_lock); */
+	vspadev->irq_bits &= ~1;
+}
+
+/* Command has been consumed or Mailbox transfer completed */
+static void vspa_flags0_irq_handler(struct vspa_device *vspadev)
+{
+	int seqid;
+	uint32_t msb, lsb;
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t status = vspa_reg_read(regs + STATUS_REG_OFFSET);
+	uint32_t flags0 = vspa_reg_read(regs + HOST_FLAGS0_REG_OFFSET);
+
+	vspa_reg_write(regs + HOST_FLAGS0_REG_OFFSET, flags0);
+
+	/* spin_lock(&vspadev->irq_lock); */
+	if (vspadev->irq_bits) {
+		pr_err("VSPA%d flg0 irqs = %d\n",
+			vspadev->id, vspadev->irq_bits);
+	}
+	vspadev->irq_bits |= 2;
+
+	IF_DEBUG(DEBUG_FLAGS0_IRQ)
+		pr_info("vspa%d: flags0 = %08x, status = %08x\n",
+			vspadev->id, flags0, status);
+#ifdef MB_CHECK_IN_IRQ
+	/* Check if any queued mailbox transactions completed */
+	mb_transmit_check(vspadev);
+#endif
+
+	/* Mailbox consumed handshake (per AVI) */
+	if (status & STATUS_REG_IRQ_VCPU_READ_MSG) {
+		/* Check if any queued mailbox transactions completed */
+		IF_DEBUG(DEBUG_MBOX32_OUT | DEBUG_MBOX64_OUT)
+			pr_info("vspa%d: mailbox consumed handshake\n",
+					vspadev->id);
+		mb_transmit_check(vspadev);
+	}
+
+	/* Handle Mailbox interrupts */
+	if (status & STATUS_REG_IRQ_VCPU_SENT_MSG) {
+		status = vspa_reg_read(regs + HOST_MBOX_STATUS_REG_OFFSET);
+		IF_DEBUG(DEBUG_FLAGS0_IRQ)
+			pr_info("mbox status = %08x\n", status);
+
+		if (status & MBOX_STATUS_IN_1_64_BIT) {
+			msb = vspa_reg_read(regs + HOST_IN_1_64_MSB_REG_OFFSET);
+			lsb = vspa_reg_read(regs + HOST_IN_1_64_LSB_REG_OFFSET);
+			IF_DEBUG(DEBUG_MBOX64_IN)
+				pr_info("vspa%d: mbox64 in %08X %08X\n",
+					vspadev->id, msb, lsb);
+			event_enqueue(vspadev, VSPA_EVENT_MB64_IN, 0, 0,
+								msb, lsb);
+		}
+		vspa_reg_write(regs + STATUS_REG_OFFSET,
+				STATUS_REG_IRQ_VCPU_SENT_MSG);
+	}
+
+	/* Handle Flags0 interrupts */
+	while (flags0) {
+		seqid = ffs(flags0) - 1;
+		pr_debug("flags0 = %08X, seqid = %d\n", flags0, seqid);
+		flags0 &= ~(1 << seqid);
+		if (vspadev->seqid[seqid].cmd_id < 0) {
+			ERR("%d: unmatched seqid %d - flag ignored\n",
+				vspadev->id, seqid);
+			/* TODO - report error event */
+			continue;
+		}
+		/* Optionally send CMD consumed event message */
+		if (vspadev->seqid[seqid].flags & VSPA_FLAG_REPORT_CMD_CONSUMED)
+			event_enqueue(vspadev, VSPA_EVENT_CMD,
+				      vspadev->seqid[seqid].cmd_id, 0,
+				      vspadev->seqid[seqid].payload1,
+				      vspadev->seqid[seqid].reply_size);
+		/* Release SEQID if no reply is expected */
+		if (!(vspadev->seqid[seqid].flags & VSPA_FLAG_EXPECT_CMD_REPLY))
+			seqid_release(vspadev, seqid);
+		else { /* otherwise just release the command buffer */
+			if (vspadev->seqid[seqid].cmd_bd_index >= 0) {
+				pool_free_bd(&vspadev->cmd_pool,
+				     vspadev->seqid[seqid].cmd_bd_index);
+				vspadev->seqid[seqid].cmd_bd_index = -1;
+			}
+		}
+
+	}
+
+	/* spin_unlock(&vspadev->irq_lock); */
+	vspadev->irq_bits &= ~2;
+}
+
+static void vspa_dma_irq_handler(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	struct dma_queue *dq = &(vspadev->dma_queue);
+	struct vspa_dma_req *dr;
+	int flags = 0;
+	int spm_flags = 0;
+	u32 status;
+	u32 stat;
+	u32 mask;
+	u32 spm_mask;
+	int err;
+
+	if (vspadev->irq_bits) {
+		pr_err("VSPA%d dma irqs = %d\n",
+			vspadev->id, vspadev->irq_bits);
+	}
+	vspadev->irq_bits |= 4;
+
+	status = vspa_reg_read(regs + STATUS_REG_OFFSET);
+	IF_DEBUG(DEBUG_DMA_IRQ)
+	pr_info("vspa%d: dma_irq %08x, COMP %08x, XFRERR %08x, CFGERR %08x\n",
+			vspadev->id,
+			vspa_reg_read(regs + DMA_IRQ_STAT_REG_OFFSET),
+			vspa_reg_read(regs + DMA_COMP_STAT_REG_OFFSET),
+			vspa_reg_read(regs + DMA_XFRERR_STAT_REG_OFFSET),
+			vspa_reg_read(regs + DMA_CFGERR_STAT_REG_OFFSET));
+
+	mask = 1 << vspadev->dma_queue.chan;
+	/* legacy VSPA images consider all other DMA channels to be SPM */
+	if (vspadev->spm_dma_chan == VSPA_DMA_CHANNELS)
+		spm_mask = ~mask;
+	else if (vspadev->spm_dma_chan < VSPA_DMA_CHANNELS)
+		spm_mask = vspadev->spm_dma_chan < VSPA_DMA_CHANNELS ?
+					1 << vspadev->spm_dma_chan : 0;
+	else
+		spm_mask = 0;
+
+	/* Check for completed DMAs */
+	if (status & STATUS_REG_IRQ_DMA_COMP) {
+		stat = vspa_reg_read(regs + DMA_IRQ_STAT_REG_OFFSET);
+		if (stat & mask) {
+			flags |= DMA_FLAG_COMPLETE;
+			vspa_reg_write(regs + DMA_IRQ_STAT_REG_OFFSET, mask);
+			vspa_reg_write(regs + DMA_COMP_STAT_REG_OFFSET, mask);
+			stat &= ~mask;
+		}
+		if (stat & spm_mask) {
+			spm_flags |= DMA_FLAG_COMPLETE;
+			vspa_reg_write(regs+DMA_IRQ_STAT_REG_OFFSET, spm_mask);
+			vspa_reg_write(regs+DMA_COMP_STAT_REG_OFFSET, spm_mask);
+			stat &= ~spm_mask;
+		}
+		/* Watch for completed DMAs from VSPA with incorrect IRQ_EN */
+		if (stat) {
+			ERR("%d: DMA IRQ STAT %08x\n", vspadev->id, stat);
+			vspa_reg_write(regs + DMA_IRQ_STAT_REG_OFFSET, stat);
+		}
+	}
+	/* Check for DMA errors from any channel */
+	if (status & STATUS_REG_IRQ_DMA_ERR) {
+		/* DMA transfer errors */
+		stat = vspa_reg_read(regs + DMA_XFRERR_STAT_REG_OFFSET);
+		if (stat) {
+			if (stat & mask) /* Transfer error from DMA channel */
+				flags |= DMA_FLAG_XFRERR;
+			if (stat & spm_mask) /* Transfer error from SPM DMA */
+				spm_flags |= DMA_FLAG_XFRERR;
+			if (stat & ~(mask | spm_mask)) { /* Other channels */
+				ERR("%d: DMA XFRERR %08x\n", vspadev->id, stat);
+			}
+			vspa_reg_write(regs+DMA_XFRERR_STAT_REG_OFFSET, stat);
+		}
+		/* DMA configuration errors */
+		stat = vspa_reg_read(regs + DMA_CFGERR_STAT_REG_OFFSET);
+		if (stat) {
+			if (stat & mask) /* Config error from DMA channel */
+				flags |= DMA_FLAG_CFGERR;
+			if (stat & spm_mask) /* Config error from SPM DMA */
+				spm_flags |= DMA_FLAG_CFGERR;
+			if (stat & ~(mask | spm_mask)) { /* Other channels */
+				ERR("%d: DMA CFGERR %08x\n", vspadev->id, stat);
+			}
+			vspa_reg_write(regs+DMA_CFGERR_STAT_REG_OFFSET, stat);
+		}
+	}
+
+	if (flags) {
+		dr = &(dq->entry[dq->idx_chk]);
+		if (dr->type == VSPA_EVENT_DMA) {
+			err = 0;
+			if (flags & DMA_FLAG_CFGERR)
+				err = VSPA_ERR_DMA_CFGERR;
+			else if (flags & DMA_FLAG_XFRERR)
+				err = VSPA_ERR_DMA_XFRERR;
+			if (dr->flags & VSPA_FLAG_REPORT_DMA_COMPLETE || err) {
+				event_enqueue(vspadev, VSPA_EVENT_DMA, dr->id,
+					err, dr->dmem_addr, dr->byte_cnt);
+			}
+		} else if (dr->type == VSPA_EVENT_CMD) {
+			if (dr->id >= MAX_SEQIDS)
+				; /* skip multiple DMAs */
+			else
+				if (vspadev->seqid[dr->id].cmd_buffer_idx >= 0) {
+					cbuffer_free(&vspadev->cmd_buffer,
+					vspadev->seqid[dr->id].cmd_buffer_idx,
+					vspadev->seqid[dr->id].cmd_buffer_size
+					);
+					vspadev->seqid[dr->id].cmd_buffer_idx = -1;
+			}
+		} else
+			ERR("%d: unknown DMA type %d\n", vspadev->id, dr->type);
+		dq->idx_chk = dq->idx_dma;
+		dma_transmit(vspadev);
+	}
+
+	if (spm_flags)
+		spm_update(vspadev, spm_flags);
+
+	/* spin_unlock(&vspadev->irq_lock); */
+	vspadev->irq_bits &= ~4;
+}
+
+static void vspa_gen_irq_handler(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t irqen  = vspa_reg_read(regs + IRQEN_REG_OFFSET);
+	uint32_t status = vspa_reg_read(regs + STATUS_REG_OFFSET);
+
+	/* spin_lock(&vspadev->irq_lock); */
+	if (vspadev->irq_bits) {
+		pr_err("VSPA%d gen irqs = %d\n",
+			vspadev->id, vspadev->irq_bits);
+	}
+	vspadev->irq_bits |= 8;
+
+	vspa_reg_write(regs + STATUS_REG_OFFSET, status);
+	pr_info("vspa%d: IRQEN %08x, STATUS %08x => %08X\n",
+		vspadev->id, irqen, status,
+		vspa_reg_read(regs + STATUS_REG_OFFSET));
+
+	/* spin_unlock(&vspadev->irq_lock); */
+	vspadev->irq_bits &= ~8;
+}
+
+static irqreturn_t vspa_irq_handler(int irq, void *dev)
+{
+	struct vspa_device *vspadev = (struct vspa_device *)dev;
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t status = vspa_reg_read(regs + STATUS_REG_OFFSET);
+
+	pr_debug("vspa%d: status %08x\n", vspadev->id, status);
+
+	if (status & STATUS_REG_IRQ_NON_GEN) {
+		if ((status & STATUS_REG_IRQ_FLAGS0) ||
+			status & STATUS_REG_IRQ_VCPU_MSG)
+			vspa_flags0_irq_handler(vspadev);
+		if (status & STATUS_REG_IRQ_FLAGS1)
+			vspa_flags1_irq_handler(vspadev);
+		if ((status & STATUS_REG_IRQ_DMA_COMP) ||
+			(status & STATUS_REG_IRQ_DMA_ERR))
+			vspa_dma_irq_handler(vspadev);
+	} else
+		vspa_gen_irq_handler(vspadev);
+	return IRQ_HANDLED;
+}
+
+/************************ Power Up / Down *************************/
+static int powerdown(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	u32 __iomem *dbg_regs = vspadev->dbg_regs;
+	int ret = 0, ret1 = 0;
+	uint32_t val;
+	int ctr;
+
+	/* Disable all interrupts */
+	vspa_reg_write(regs + IRQEN_REG_OFFSET, 0x0);
+
+	vspadev->versions.vspa_sw_version = ~0;
+	vspadev->versions.ippu_sw_version = ~0;
+	vspadev->eld_filename[0] = '\0';
+	vspadev->watchdog_interval_msecs = VSPA_WATCHDOG_INTERVAL_DEFAULT;
+
+	if (vspadev->state == VSPA_STATE_RUNNING_IDLE) {
+		/* clear debug_msg_go, host_msg_go, ru_go and host_go bits */
+		val = vspa_reg_read(regs + CONTROL_REG_OFFSET);
+		val = (val & CONTROL_REG_MASK) & ~CONTROL_HOST_GO;
+		vspa_reg_write(regs + CONTROL_REG_OFFSET, val);
+
+		/* Enable the invasive (halting) debug mode */
+		vspa_reg_write(dbg_regs + DBG_GDBEN_REG_OFFSET, 0x1);
+
+		/* Enable dbg_dbgen */
+		val = vspa_reg_read(dbg_regs + DBG_DVR_REG_OFFSET);
+		val = val | (1 << 13);
+		vspa_reg_write(dbg_regs + DBG_DVR_REG_OFFSET, val);
+
+		/* change access perpective to VCPU */
+		val = vspa_reg_read(dbg_regs + DBG_RAVAP_REG_OFFSET);
+		val = val | (1 << 31);
+		vspa_reg_write(dbg_regs + DBG_RAVAP_REG_OFFSET, val);
+
+		val = vspa_reg_read(regs + CONTROL_REG_OFFSET);
+		val = (val & CONTROL_REG_MASK) &
+			~(CONTROL_IPPU_GO | CONTROL_HOST_MSG_GO
+					| CONTROL_DEBUG_MSG_GO);
+
+		/* change access perpective back to Host */
+		val = vspa_reg_read(dbg_regs + DBG_RAVAP_REG_OFFSET);
+		val = val & ~(1 << 31);
+		vspa_reg_write(dbg_regs + DBG_RAVAP_REG_OFFSET, val);
+
+		/* Stop all VSPA activities using force_halt */
+		vspa_reg_write(dbg_regs + DBG_RCR_REG_OFFSET, 0x4);
+
+		/* Wait for the halted bit to be set */
+		for (ctr = VSPA_HALT_TIMEOUT; ctr; ctr--) {
+			udelay(1);
+			ret1 = vspa_reg_read(dbg_regs + DBG_RCSTATUS_REG_OFFSET)
+				& (1 << 13);
+			if (ret1)
+				break;
+		}
+		if (!(ret1)) {
+			ERR("%d: powerdown() timeout waiting for halt\n",
+					vspadev->id);
+			ret = -ETIME;
+			vspadev->state = VSPA_STATE_UNKNOWN;
+			return ret;
+		}
+
+		/* Issue the reset */
+		val = vspa_reg_read(regs + CONTROL_REG_OFFSET);
+		val = (val & CONTROL_REG_MASK) | CONTROL_VCPU_RESET;
+		vspa_reg_write(regs + CONTROL_REG_OFFSET, val);
+
+		/* Start VSPA activities using resume */
+		vspa_reg_write(dbg_regs + DBG_RCR_REG_OFFSET, 0x2);
+	}
+
+	/* shut the timer down */
+	mod_timer(&vspadev->watchdog_timer, jiffies);
+
+	vspadev->state = ret ? VSPA_STATE_UNKNOWN : VSPA_STATE_POWER_DOWN;
+	return ret;
+}
+
+static int powerup(struct vspa_device *vspadev)
+{
+	int ret = 0;
+	u32 __iomem *regs = vspadev->regs;
+
+	if (vspadev->state != VSPA_STATE_POWER_DOWN)
+		return -EPERM;
+
+	/* Disable all interrupts */
+	vspa_reg_write(regs + IRQEN_REG_OFFSET, 0x0);
+	dma_reset_queue(vspadev);
+	event_reset_queue(vspadev);
+	mbox_reset(vspadev);
+	cmd_reset(vspadev);
+
+	vspadev->state = ret ? VSPA_STATE_UNKNOWN :
+					VSPA_STATE_UNPROGRAMMED_IDLE;
+	return ret;
+}
+
+static int startup(struct vspa_device *vspadev)
+{
+	u32 __iomem *regs = vspadev->regs;
+	uint32_t val, msb, lsb;
+	uint32_t dma_channels;
+	uint32_t vspa_sw_version, ippu_sw_version;
+	int ctr;
+
+	if (vspadev->state != VSPA_STATE_LOADING)
+		return -EPERM;
+
+	IF_DEBUG(DEBUG_STARTUP) {
+		pr_info("vspa%d: startup()\n", vspadev->id);
+		pr_info("Filename: '%s'\n", vspadev->eld_filename);
+		pr_info("Command buffer: addr = %08X, bytes = %08X\n",
+			vspadev->cmdbuf_addr, vspadev->cmd_pool.size * 4);
+		pr_info("SPM %s buffer: addr = %p, bytes = %08X\n",
+			vspadev->spm_user_buf ? "User" : "Driver",
+			vspadev->spm_buf_paddr, vspadev->spm_buf_bytes);
+	}
+
+	/* Ask the VSPA to go */
+	val = vspa_reg_read(regs + CONTROL_REG_OFFSET);
+	val = (val & CONTROL_REG_MASK) | CONTROL_HOST_GO;
+	vspa_reg_write(regs + CONTROL_REG_OFFSET, val);
+
+	/* Wait for the 64 bit mailbox bit to be set */
+	for (ctr = VSPA_STARTUP_TIMEOUT; ctr; ctr--) {
+		if (vspa_reg_read(regs + HOST_MBOX_STATUS_REG_OFFSET) &
+							MBOX_STATUS_IN_64_BIT)
+			break;
+		udelay(1);
+	}
+	if (!ctr) {
+		ERR("%d: timeout waiting for Boot Complete msg\n", vspadev->id);
+		goto startup_fail;
+	}
+	msb = vspa_reg_read(regs + HOST_IN_64_MSB_REG_OFFSET);
+	lsb = vspa_reg_read(regs + HOST_IN_64_LSB_REG_OFFSET);
+	IF_DEBUG(DEBUG_STARTUP)
+		pr_info("Boot Ok Msg: msb = %08X, lsb = %08X\n", msb, lsb);
+	/* Check Boot Complete message */
+	if (msb != 0xF1000000) {
+		ERR("%d: Boot Complete msg did not match\n", vspadev->id);
+		goto startup_fail;
+	}
+	dma_channels = lsb;
+
+	vspa_sw_version = vspa_reg_read(regs + SWVERSION_REG_OFFSET);
+	ippu_sw_version = vspa_reg_read(regs + IPPU_SWVERSION_REG_OFFSET);
+
+	/* Set SPM buffer */
+	msb = (0x70 << 24) | vspadev->spm_buf_bytes;
+	lsb = (uint32_t)(dma_addr_t)vspadev->spm_buf_paddr;
+	vspa_reg_write(regs + HOST_OUT_64_MSB_REG_OFFSET, msb);
+	vspa_reg_write(regs + HOST_OUT_64_LSB_REG_OFFSET, lsb);
+	/* Wait for the 64 bit mailbox bit to be set */
+	for (ctr = VSPA_STARTUP_TIMEOUT; ctr; ctr--) {
+		if (vspa_reg_read(regs + HOST_MBOX_STATUS_REG_OFFSET) &
+							MBOX_STATUS_IN_64_BIT)
+			break;
+		udelay(1);
+	}
+	if (!ctr) {
+		ERR("%d: timeout waiting for SPM Ack msg\n", vspadev->id);
+		goto startup_fail;
+	}
+	msb = vspa_reg_read(regs + HOST_IN_64_MSB_REG_OFFSET);
+	lsb = vspa_reg_read(regs + HOST_IN_64_LSB_REG_OFFSET);
+	IF_DEBUG(DEBUG_STARTUP)
+		pr_info("SPM Ack Msg: msb = %08X, lsb = %08X\n", msb, lsb);
+	if (msb != 0xF0700000) {
+		ERR("%d: SPM Ack error %08X\n", vspadev->id, msb);
+		goto startup_fail;
+	}
+
+	if (dma_channels) {
+		vspadev->spm_dma_chan   = (dma_channels >> 24)&0xFF;
+		vspadev->bulk_dma_chan  = (dma_channels >> 16)&0xFF;
+		vspadev->reply_dma_chan = (dma_channels >>  8)&0xFF;
+		vspadev->cmd_dma_chan   = (dma_channels)&0xFF;
+	} else { /* legacy images */
+		vspadev->spm_dma_chan   = VSPA_DMA_CHANNELS;
+		vspadev->bulk_dma_chan  = 0xFF;
+		vspadev->reply_dma_chan = 0xFF;
+		vspadev->cmd_dma_chan   = vspadev->legacy_cmd_dma_chan;
+	}
+
+	IF_DEBUG(DEBUG_STARTUP) {
+		pr_info("SW Version: vspa = %08X, ippu = %08X\n",
+				vspa_sw_version, ippu_sw_version);
+		pr_info("DMA chan: spm %02X, bulk %02X, reply %02X, cmd %02X\n",
+			vspadev->spm_dma_chan, vspadev->bulk_dma_chan,
+			vspadev->reply_dma_chan, vspadev->cmd_dma_chan);
+	}
+
+	vspadev->versions.vspa_sw_version = vspa_sw_version;
+	vspadev->versions.ippu_sw_version = ippu_sw_version;
+	vspadev->watchdog_value =
+			~(vspa_reg_read(regs + GP_OUT3_REG_OFFSET) >> 16);
+	vspadev->state = VSPA_STATE_RUNNING_IDLE;
+	vspa_enable_mailbox_irqs(vspadev);
+	init_completion(&vspadev->watchdog_complete);
+	watchdog_callback((unsigned long) vspadev);
+	return 0;
+
+startup_fail:
+	vspadev->versions.vspa_sw_version = ~0;
+	vspadev->versions.ippu_sw_version = ~0;
+	vspadev->state = VSPA_STATE_STARTUP_ERR;
+	vspa_reg_write(vspadev->regs + IRQEN_REG_OFFSET, 0);
+	return -EIO;
+}
+
+int full_state(struct vspa_device *vspadev)
+{
+	int state = vspadev->state;
+
+	if (state == VSPA_STATE_UNPROGRAMMED_IDLE ||
+	    state == VSPA_STATE_RUNNING_IDLE) {
+		if (vspa_reg_read(vspadev->regs + STATUS_REG_OFFSET) & 0x100)
+			state++;
+	}
+	return state;
+}
+
+/************************** System Functions ************************/
+
+static int vspa_open(struct inode *inode, struct file *fp)
+{
+	struct vspa_device *vspadev = NULL;
+	int rc = 0;
+
+	vspadev = container_of(inode->i_cdev, struct vspa_device, cdev);
+	if (vspadev != NULL) {
+		fp->private_data = vspadev;
+		event_list_update(vspadev);
+	} else {
+		dev_err(vspadev->dev, "No device context found for %s %d\n",
+					VSPA_DEVICE_NAME, iminor(inode));
+
+		rc = -ENODEV;
+	}
+
+	return rc;
+}
+
+static int vspa_release(struct inode *inode, struct file *fp)
+{
+	struct vspa_device *vspadev = (struct vspa_device *)fp->private_data;
+	int rc = 0;
+
+	if (vspadev != NULL) {
+		event_list_update(vspadev);
+	} else {
+		pr_err("No device context found for %s %d\n",
+					VSPA_DEVICE_NAME, iminor(inode));
+		rc = -ENODEV;
+	}
+
+	return 0;
+}
+
+static long vspa_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	int rc = 0;
+	struct vspa_device *vspadev = (struct vspa_device *)fp->private_data;
+	u32 __iomem *regs = vspadev->regs;
+	u32 __iomem *dbg_regs = vspadev->dbg_regs;
+	struct vspa_dma_req dma_req;
+	struct vspa_startup startup_desc;
+	struct vspa_reg reg;
+	struct vspa_mb64 mb64;
+	struct vspa_event_read evt_rd;
+	uint32_t align_bytes = vspadev->hardware.axi_data_width >> 3;
+	int state;
+
+
+	IF_DEBUG(DEBUG_IOCTL)
+		pr_info("vspa%d: cmd %08x, arg %08lx\n", vspadev->id, cmd, arg);
+
+	/* Ignore invalid commands */
+	if (_IOC_TYPE(cmd) != VSPA_MAGIC_NUM)
+		return -ENOTTY;
+	if (_IOC_NR(cmd)   >  VSPA_IOC_MAX)
+		return -ENOTTY;
+
+	/* Check user space memory access is valid */
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		rc = !access_ok(VERIFY_WRITE, (void *)arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		rc = !access_ok(VERIFY_READ, (void *)arg, _IOC_SIZE(cmd));
+	if (rc)
+		return -EFAULT;
+
+	event_list_update(vspadev);
+
+	switch (cmd) {
+	case VSPA_IOC_GET_HW_CFG:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_GET_HW_CFG\n", vspadev->id);
+
+		rc = copy_to_user((struct vspa_hardware *)arg,
+			&vspadev->hardware, sizeof(struct vspa_hardware));
+		if (rc)
+			rc = -EFAULT;
+		break;
+
+	case VSPA_IOC_GET_STATE:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_GET_STATE\n", vspadev->id);
+
+		state = full_state(vspadev);
+		rc = __put_user(state, (int *)arg);
+		if (rc)
+			rc = -EFAULT;
+		break;
+
+	case VSPA_IOC_REG_READ:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_REG_READ\n", vspadev->id);
+
+		rc = copy_from_user(&reg, (struct vspa_reg *)arg, sizeof(reg));
+		if (rc)
+			rc = -EFAULT;
+		else if (reg.reg >= 0x400)
+			rc = -EINVAL;
+		else {
+			uint32_t r = reg.reg;
+
+			if (r < 0x200)
+				reg.val = vspa_reg_read(regs + r);
+			else
+				reg.val = vspa_reg_read(dbg_regs + (r&0x1FF));
+			rc = copy_to_user((struct vspa_reg *)arg, &reg,
+								sizeof(reg));
+			if (rc)
+				rc = -EFAULT;
+		}
+		break;
+
+	case VSPA_IOC_REG_WRITE:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_REG_WRITE\n", vspadev->id);
+
+		rc = copy_from_user(&reg, (struct vspa_reg *)arg, sizeof(reg));
+		if (rc)
+			rc = -EFAULT;
+		else if (reg.reg >= 0x400)
+			rc = -EINVAL;
+		else {
+			uint32_t r = reg.reg;
+
+			if (r < 0x200)
+				vspa_reg_write(regs + r, reg.val);
+			else
+				vspa_reg_write(dbg_regs + (r & 0x1FF),
+								reg.val);
+		}
+		break;
+
+	case VSPA_IOC_GET_VERSIONS:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_GET_VERSIONS\n", vspadev->id);
+
+		rc = copy_to_user((struct vspa_versionsi *)arg,
+				  &vspadev->versions,
+				  sizeof(struct vspa_versions));
+		if (rc)
+			rc = -EFAULT;
+		break;
+
+	case VSPA_IOC_REQ_POWER:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_REQ_POWER\n", vspadev->id);
+
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: power %ld\n", vspadev->id, arg);
+		spin_lock(&vspadev->control_lock);
+		switch (arg) {
+		case VSPA_POWER_DOWN:
+			rc = powerdown(vspadev);
+			break;
+		case VSPA_POWER_CYCLE:
+			rc = powerdown(vspadev); /*FALLTHROUGH*/
+		case VSPA_POWER_UP:
+			if (rc == 0)
+				rc = powerup(vspadev);
+			break;
+		default:
+			rc = -EINVAL;
+			break;
+		}
+		spin_unlock(&vspadev->control_lock);
+		break;
+
+	case VSPA_IOC_DMA:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_DMA\n", vspadev->id);
+
+		if (vspadev->state == VSPA_STATE_UNPROGRAMMED_IDLE) {
+			vspadev->state = VSPA_STATE_LOADING;
+			/* Enable DMA error and DMA complete interrupts */
+			vspa_enable_dma_irqs(vspadev);
+		} else if (vspadev->state < VSPA_STATE_LOADING)
+			return -EPERM;
+
+		rc = copy_from_user(&dma_req, (struct vspa_dma_req *)arg,
+							sizeof(dma_req));
+		if (rc)
+			rc = -EFAULT;
+		else {
+			if ((dma_req.dmem_addr & (align_bytes - 1)) ||
+			    (dma_req.axi_addr & (align_bytes - 1))) {
+				rc = -EINVAL;
+			} else {
+
+				dma_req.type = VSPA_EVENT_DMA;
+				dma_req.xfr_ctrl = (dma_req.xfr_ctrl & ~0x1F) |
+					vspadev->cmd_dma_chan;
+				rc = dma_enqueue(vspadev, &dma_req);
+			}
+		}
+
+		pr_debug("vspa%d: ctrl %08x, dmem %08x, axi %08llx, cnt %08x,",
+				vspadev->id, dma_req.control,
+				dma_req.dmem_addr, dma_req.axi_addr,
+				dma_req.byte_cnt);
+		pr_debug(" ctrl %08x\n", dma_req.xfr_ctrl);
+
+		break;
+
+	case VSPA_IOC_STARTUP:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_STARTUP\n", vspadev->id);
+
+		if (vspadev->state != VSPA_STATE_LOADING)
+			return -EPERM;
+
+		rc = copy_from_user(&startup_desc,
+			(struct vspa_startup *)arg, sizeof(startup_desc));
+		if (rc)
+			rc = -EFAULT;
+		else
+			if ((startup_desc.cmd_buf_size & (align_bytes - 1)) ||
+				(startup_desc.cmd_buf_addr & (align_bytes - 1)))
+				rc = -EINVAL;
+			else
+				if (startup_desc.spm_buf_size && (
+						(startup_desc.spm_buf_size &
+						(align_bytes - 1)) ||
+						(startup_desc.spm_buf_addr &
+						(align_bytes - 1))))
+					rc = -EINVAL;
+		if (rc)
+			break;
+
+		spin_lock(&vspadev->control_lock);
+		IF_DEBUG(DEBUG_IOCTL) {
+			pr_debug("vspa%d: cmd_addr %08x, cmd_size %08x, ",
+				vspadev->id, startup_desc.cmd_buf_addr,
+				startup_desc.cmd_buf_size);
+			pr_debug(" cmd_dma_chan %d, file %s\n",
+				startup_desc.cmd_dma_chan,
+				startup_desc.filename);
+		}
+
+		vspadev->legacy_cmd_dma_chan = startup_desc.cmd_dma_chan;
+		vspadev->cmdbuf_addr = startup_desc.cmd_buf_addr;
+		vspadev->cmd_pool.size = startup_desc.cmd_buf_size / 4;
+		if (vspadev->spm_user_buf) {
+			vspadev->spm_user_buf = 0;
+			iounmap(vspadev->spm_buf_vaddr);
+		}
+		if (startup_desc.spm_buf_size) {
+			vspadev->spm_user_buf = 1;
+			vspadev->spm_buf_bytes = startup_desc.spm_buf_size;
+			vspadev->spm_buf_paddr =
+					(uint32_t *)(dma_addr_t)
+					startup_desc.spm_buf_addr;
+			vspadev->spm_buf_vaddr = ioremap(
+					(dma_addr_t)vspadev->spm_buf_paddr,
+					vspadev->spm_buf_bytes);
+		} else {
+			vspadev->spm_buf_bytes = vspadev->spm_buffer_bytes;
+			vspadev->spm_buf_paddr = vspadev->spm_buffer_paddr;
+			vspadev->spm_buf_vaddr = vspadev->spm_buffer_vaddr;
+		}
+		vspadev->spm_addr = (vspadev->spm_buf_bytes - 8) >> 3;
+		if (vspadev->spm_buf_bytes > 0)
+			memset(vspadev->spm_buf_vaddr, 0,
+					vspadev->spm_buf_bytes);
+
+		vspadev->watchdog_interval_msecs =
+						VSPA_WATCHDOG_INTERVAL_DEFAULT;
+		strncpy(vspadev->eld_filename, startup_desc.filename,
+							VSPA_MAX_ELD_FILENAME);
+		vspadev->eld_filename[VSPA_MAX_ELD_FILENAME-1] = '\0';
+		pool_reset(&vspadev->cmd_pool);
+		rc = startup(vspadev);
+		spin_unlock(&vspadev->control_lock);
+		break;
+
+	case VSPA_IOC_MB64_WRITE:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_MB64_WRITE\n", vspadev->id);
+
+		if (vspadev->state != VSPA_STATE_RUNNING_IDLE)
+			return -EPERM;
+		rc = copy_from_user(&mb64,
+			(struct vspa_mb64 *)arg, sizeof(mb64));
+		if (rc)
+			rc = -EFAULT;
+		else {
+			int next_slot;
+#ifdef MB_CHECK_ON_WRITE
+			/* Check if any queued mailbox transactions completed */
+			mb_transmit_check(vspadev);
+#endif
+			spin_lock(&vspadev->control_lock);
+			next_slot = mbox_next_slot(&vspadev->mb64_queue);
+			if (next_slot < 0) {
+				ERR("%d: mb64 queue full\n", vspadev->id);
+				rc = -ENOMEM;
+			} else {
+				if (next_slot == 0)
+					vspadev->mb64[MBOX_QUEUE_ENTRIES - 1]
+						= mb64;
+				else
+					vspadev->mb64[next_slot - 1] = mb64;
+				vspadev->mb64_queue.idx_enqueue = next_slot;
+
+				IF_DEBUG(DEBUG_MBOX64_OUT) {
+					pr_debug("vspa%d: mbox64 out idx %d: ",
+						vspadev->id, next_slot ?
+						next_slot - 1 :
+						MBOX_QUEUE_ENTRIES - 1);
+					pr_debug("%08x %08x queued\n",
+							mb64.data_msb,
+							mb64.data_lsb);
+				}
+				mb64_transmit(vspadev);
+#ifdef MB_CHECK_TIMER
+				mod_timer(&vspadev->mbchk_timer, jiffies + 1);
+#endif
+			}
+			spin_unlock(&vspadev->control_lock);
+		}
+		break;
+
+	case VSPA_IOC_WATCHDOG_INT:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_WATCHDOG_INT\n", vspadev->id);
+
+		if (arg > VSPA_WATCHDOG_INTERVAL_MAX)
+			rc = -EINVAL;
+		else {
+			if (arg == 0)
+				arg = VSPA_WATCHDOG_INTERVAL_DEFAULT;
+			else
+				if (arg < VSPA_WATCHDOG_INTERVAL_MIN)
+					arg = VSPA_WATCHDOG_INTERVAL_MIN;
+			vspadev->watchdog_interval_msecs = arg;
+			if (vspadev->state == VSPA_STATE_RUNNING_IDLE) {
+				mod_timer(&vspadev->watchdog_timer,
+					jiffies + msecs_to_jiffies(arg));
+			}
+		}
+		break;
+
+	case VSPA_IOC_SET_POLL_MASK:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_SET_POLL_MASK\n",
+					vspadev->id);
+
+		arg &= VSPA_MSG_ALL_EVENTS;
+		vspadev->poll_mask = arg ? arg : VSPA_MSG_ALL_EVENTS;
+		break;
+
+	case VSPA_IOC_EVENT_READ:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_EVENT_READ\n", vspadev->id);
+
+		rc = copy_from_user(&evt_rd,
+			(struct vspa_event_read *)arg, sizeof(evt_rd));
+		if (rc)
+			rc = -EFAULT;
+		else {
+			rc = read_event(vspadev, &evt_rd);
+			pr_debug("read_event mask %04X, rc %d\n",
+					evt_rd.event_mask, rc);
+			if (rc > 0)
+				rc = 0;
+		}
+		break;
+
+	case VSPA_IOC_GET_DEBUG:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_GET_DEBUG\n", vspadev->id);
+
+		rc = __put_user(vspadev->debug, (int *)arg);
+		if (rc)
+			rc = -EFAULT;
+		break;
+
+	case VSPA_IOC_SET_DEBUG:
+		IF_DEBUG(DEBUG_IOCTL)
+			pr_info("vspa%d: VSPA_IOC_SET_DEBUG\n", vspadev->id);
+
+		vspadev->debug = arg;
+		break;
+
+	default:
+		rc = -ENOTTY;
+		break;
+	}
+
+	return rc;
+}
+
+static unsigned int vspa_poll(struct file *fp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct vspa_device *vspadev = (struct vspa_device *)fp->private_data;
+
+	event_list_update(vspadev);
+
+	spin_lock(&vspadev->event_list_lock);
+
+	poll_wait(fp, &vspadev->event_wait_q, wait);
+
+	if (vspadev->state == VSPA_STATE_RUNNING_IDLE)
+		mask |= (POLLOUT | POLLWRNORM);
+	if (vspadev->event_list_mask & vspadev->poll_mask)
+		mask |= (POLLIN | POLLRDNORM);
+	else if (vspadev->state < VSPA_STATE_LOADING)
+		mask = POLLHUP;
+
+	spin_unlock(&vspadev->event_list_lock);
+
+	return mask;
+}
+
+static ssize_t vspa_read(struct file *fp, char __user *buf,
+			size_t len, loff_t *off)
+{
+	struct vspa_device *vspadev = (struct vspa_device *)fp->private_data;
+	struct vspa_event_read evt_rd;
+	int ret;
+
+	evt_rd.event_mask = (dma_addr_t)off;
+	evt_rd.timeout = (fp->f_flags & O_NONBLOCK) ? 0 : -1;
+	evt_rd.buf_len = len;
+	evt_rd.buf_ptr = (struct vspa_event *)buf;
+
+	ret = read_event(vspadev, &evt_rd);
+	return (ret == -ENODATA) ? 0 : ret;
+}
+
+static ssize_t vspa_write(struct file *fp, const char __user *buf,
+			 size_t len, loff_t *off)
+{
+	struct vspa_device *vspadev = (struct vspa_device *)fp->private_data;
+	int err;
+	int8_t index;
+	uint32_t *dmabuf;
+	int seqid;
+	int words;
+	struct vspa_dma_req dr;
+	int i;
+	int start_written = 1;
+	uint32_t start_addr;
+	int cb_size;
+	int cb_idx;
+	int cmd_id = 0;
+	uint32_t flags = 0;
+	int reply_idx = -1;
+	int align_bytes = vspadev->hardware.axi_data_width >> 3;
+	int align_words = vspadev->hardware.axi_data_width >> 5;
+
+	IF_DEBUG(DEBUG_IOCTL)
+		pr_info("vspa%d: vspa_write() len = %zu\n", vspadev->id, len);
+
+	event_list_update(vspadev);
+
+	if (vspadev->state != VSPA_STATE_RUNNING_IDLE)
+		return -EPERM;
+
+	if (len > CMD_MAX_SZ_BYTES) {
+		ERR("%d: command exceeds %d bytes\n", vspadev->id,
+							CMD_MAX_SZ_BYTES);
+		return -EMSGSIZE;
+	}
+
+	if ((len & 3) != 0 || len == 0) {
+		ERR("%d: command must be multiple of 4 bytes and non zero\n",
+							 vspadev->id);
+		return -EINVAL;
+	}
+
+	spin_lock(&vspadev->control_lock);
+	seqid = seqid_get_next(vspadev);
+	if (seqid < 0) {
+		ERR("%d: no sequence id available\n", vspadev->id);
+		err = -ENOSR;
+		goto seqid_fail;
+	}
+
+	pool_consolidate(&vspadev->cmd_pool, vspadev->last_wstart);
+	pool_consolidate(&vspadev->reply_pool, -1);
+
+	words = len/4;
+	if (vspadev->first_cmd)
+		words++;
+	words = align_words * ((words + align_words - 1) / align_words);
+	index = pool_get_bd(&vspadev->cmd_pool, words);
+	IF_DEBUG(DEBUG_CMD_BD)
+		pool_print(&vspadev->cmd_pool);
+	if (index < 0) {
+		ERR("%d: no cmd pool bd available\n", vspadev->id);
+		err = -ENOSPC;
+		goto seqid_release;
+	}
+
+	cb_size = words + 4;
+	cb_idx = cbuffer_add(&vspadev->cmd_buffer, cb_size);
+	if (cb_idx < 0) {
+		ERR("%d: no cmd buffer space available\n", vspadev->id);
+		err = -ENOBUFS;
+		goto pool_free;
+	}
+
+	start_addr = vspadev->cmd_pool.bd[index].wstart + 1;
+	dmabuf = &vspadev->cmd_buffer.vaddr[cb_idx];
+	/* on failure, set the len to 0 to return empty packet to the device */
+	err = copy_from_user(dmabuf, buf, len);
+	if (err) {
+		ERR("%d: write() copy_from_user() failed\n", vspadev->id);
+	} else {
+		flags = dmabuf[0] & 0xFF;
+		if (flags & VSPA_FLAG_REPORT_CMD_REPLY)
+			flags |= VSPA_FLAG_EXPECT_CMD_REPLY;
+		if ((dmabuf[2] > 0 && dmabuf[3] == 0) ||
+		    (dmabuf[3] & (align_bytes - 1)) ||
+		    (dmabuf[5] & (align_bytes - 1))) {
+			ERR("%d: buffers must be AXI aligned\n", vspadev->id);
+			err = -EINVAL;
+			goto pool_free;
+		}
+		if (dmabuf[4] > 0) {
+			flags |= VSPA_FLAG_EXPECT_CMD_REPLY;
+			if (dmabuf[5] == 0) {
+				int reply_words = (dmabuf[4]+3)>>2;
+
+				reply_words = align_words * ((reply_words +
+						align_words - 1) / align_words);
+				reply_idx = pool_get_bd(&vspadev->reply_pool,
+								 reply_words);
+				IF_DEBUG(DEBUG_REPLY_BD)
+					pool_print(&vspadev->reply_pool);
+				if (reply_idx < 0) {
+					ERR("%d: no reply pool bd available\n",
+						vspadev->id);
+					err = -ENOSPC;
+					goto pool_free;
+				}
+				dmabuf[5] = (dma_addr_t)
+					&vspadev->reply_pool.paddr[
+				vspadev->reply_pool.bd[reply_idx].wstart];
+			}
+		}
+		cmd_id = dmabuf[1] >> 24;
+		vspadev->seqid[seqid].flags = flags;
+		vspadev->seqid[seqid].cmd_id = cmd_id;
+		vspadev->seqid[seqid].cmd_buffer_idx = cb_idx;
+		vspadev->seqid[seqid].cmd_buffer_size = cb_size;
+		vspadev->seqid[seqid].cmd_bd_index = index;
+		vspadev->seqid[seqid].reply_bd_index = reply_idx;
+		vspadev->seqid[seqid].reply_size = dmabuf[4];
+		vspadev->seqid[seqid].payload1 = dmabuf[6];
+
+		dr.dmem_addr = vspadev->cmd_pool.bd[index].wstart * 4 +
+							vspadev->cmdbuf_addr;
+		dr.axi_addr  = (unsigned int)(dma_addr_t) &
+			vspadev->cmd_buffer.paddr[cb_idx];
+		dr.xfr_ctrl = 0x2000; /* VCPU_GO */
+		dr.xfr_ctrl |= vspadev->cmd_dma_chan;
+
+		dmabuf[0] = 0;
+		dmabuf[1] = (dmabuf[1] & 0x00FFFFFF) | (seqid << 24);
+		dmabuf[words] = start_addr;
+		for (i = len/4; i < words; i++) /* filler words */
+			dmabuf[i] = 0;
+		if (vspadev->first_cmd)
+			dmabuf[words-1] = start_addr;
+		else if ((vspadev->cmd_pool.bd[index].wstart + words) ==
+							vspadev->last_wstart)
+			words++;
+		else
+			start_written = 0;
+
+		dr.byte_cnt  = words * 4;
+
+		IF_DEBUG(DEBUG_CMD) {
+			pr_info("vspa%d: CmdSeqId %02X, flags %04X\n",
+				 vspadev->id, cmd_id, flags);
+			for (i = 0; i < words; i++)
+				pr_info("%02d [%02X] = %08X\n", i,
+					vspadev->cmd_pool.bd[index].wstart + i,
+					dmabuf[i]);
+		}
+
+		dr.type = VSPA_EVENT_CMD;
+		dr.id = start_written ? seqid : 0xFF;
+		dr.flags = flags;
+		err = dma_enqueue(vspadev, &dr);
+		if (err)
+			ERR("%d: write() dma_enqueue() failed\n", vspadev->id);
+	}
+
+	/* Start pointer location is not contiguous so do another DMA */
+	if (!err && !start_written) {
+		dr.dmem_addr = vspadev->last_wstart * 4 + vspadev->cmdbuf_addr;
+		dr.axi_addr  = (dma_addr_t)
+				&vspadev->cmd_buffer.paddr[cb_idx + words];
+		dr.byte_cnt  = 4;
+		IF_DEBUG(DEBUG_CMD)
+			pr_info("-- [%02X] = %08X\n", dr.dmem_addr / 4,
+								dmabuf[words]);
+		dr.type = VSPA_EVENT_CMD;
+		dr.id = seqid;
+		dr.flags = flags;
+		err = dma_enqueue(vspadev, &dr);
+		if (err)
+			ERR("%d: write() dma_enqueue()2 failed\n", vspadev->id);
+	}
+
+	if (err) {
+		seqid_release(vspadev, seqid);
+	} else {
+		vspadev->first_cmd = 0;
+		vspadev->last_wstart = start_addr - 1;
+	}
+	spin_unlock(&vspadev->control_lock);
+
+	return err ? err : len;
+
+pool_free:
+	pool_free_bd(&vspadev->cmd_pool, index);
+seqid_release:
+	seqid_release(vspadev, seqid);
+seqid_fail:
+	spin_unlock(&vspadev->control_lock);
+	return err;
+}
+
+static int vspa_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	unsigned long off = 0, phy_addr = 0;
+	int rc = 0;
+	struct vspa_device *vspadev = fp->private_data;
+	unsigned long start  = 0, len = 0;
+
+	if (NULL == vspadev)
+		return -ENODEV;
+
+	/* Based on Offset we decide, if we map the vspa registers
+	 * (VSPA_DBG_OFFSET) 0: VSPA debug resister set
+	 * (VSPA_REG_OFFSET) 4096: VSPA IP register set
+	 * */
+	if (vma->vm_pgoff == (VSPA_DBG_OFFSET >> PAGE_SHIFT)) {
+
+		phy_addr = (unsigned long)vspadev->dbg_addr;
+
+		/* Align the start address */
+		start = phy_addr & PAGE_MASK;
+
+		/* Align the size to PAGE Size */
+		len = PAGE_ALIGN((start & ~PAGE_MASK) + vspadev->dbg_size);
+
+		/* These are IO addresses */
+		vma->vm_flags |= VM_IO;
+
+		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	} else if (vma->vm_pgoff == (VSPA_REG_OFFSET >> PAGE_SHIFT)) {
+
+		phy_addr = (unsigned long)vspadev->mem_addr;
+
+		/* Align the start address */
+		start = phy_addr & PAGE_MASK;
+
+		/* Align the size to PAGE Size */
+		len = PAGE_ALIGN((start & ~PAGE_MASK) + vspadev->mem_size);
+
+		/* These are IO addresses */
+		vma->vm_flags |= VM_IO;
+
+		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	} else {
+		return -EINVAL;
+	}
+
+	/* Reset it, as the pg_off is used as index for the type of memory */
+	vma->vm_pgoff = 0;
+
+	off = vma->vm_pgoff << PAGE_SHIFT;
+
+	if ((vma->vm_end - vma->vm_start + off) > len)
+		return -EINVAL;
+
+	off += start;
+	vma->vm_pgoff = off >> PAGE_SHIFT;
+
+	rc = io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
+			vma->vm_end - vma->vm_start,
+			vma->vm_page_prot);
+	if (rc < 0)
+		return -EAGAIN;
+
+	return 0;
+}
+
+static const struct file_operations vspa_fops = {
+	.owner = THIS_MODULE,
+	.open = vspa_open,
+	.mmap = vspa_mmap,
+	.poll = vspa_poll,
+	.read = vspa_read,
+	.write = vspa_write,
+	.unlocked_ioctl = vspa_ioctl,
+	.release = vspa_release,
+};
+
+/************************* Probe / Remove **************************/
+
+static int parse_atu_dts(struct device_node *np, struct vspa_device *vspadev)
+{
+
+	u32 num_atu_windws, win_cnt;
+	char atu_win_str[MAX_STRING_ATU_WIN];
+	u64 atu_win_proprts[2];
+	struct device_node *atu_np;
+	struct resource atu_res;
+
+	atu_np = of_parse_phandle(np, "atu", 0);
+	if (NULL != atu_np) {
+		/* Find ATU Windows to be configured */
+		if (of_property_read_u32(atu_np, "atu_win_cnt",
+					&num_atu_windws) < 0) {
+			dev_err(vspadev->dev, "ATU windows count not found %s%d\n",
+					VSPA_DEVICE_NAME, vspadev->id);
+			return -EINVAL;
+		} else {
+			vspadev->num_atu_win = num_atu_windws;
+			for (win_cnt = 0; win_cnt < vspadev->num_atu_win; win_cnt++) {
+				snprintf(atu_win_str, MAX_STRING_ATU_WIN,
+						"atu_win_%d", (win_cnt + 1));
+				if (of_property_read_u64_array(atu_np, atu_win_str,
+							atu_win_proprts, 2) < 0) {
+					dev_err(vspadev->dev,
+							"ATU WIN[%d] properties invalid %s%d\n",
+							win_cnt, VSPA_DEVICE_NAME, vspadev->id);
+					return -EINVAL;
+				} else {
+					/* Update "vspadev" structure with ATU window
+					 * information*/
+					vspadev->vspa_atu_win[win_cnt].windw_base_add =
+						atu_win_proprts[0];
+					vspadev->vspa_atu_win[win_cnt].windw_sz =
+						atu_win_proprts[1] + VSPA_ATU_MIN_WIN_SZ;
+				}
+			} /* end for */
+		}
+	} else { /* atu_np == NULL */
+		dev_err(vspadev->dev, "ATU Node not found for %s%d\n",
+				VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	}
+
+	if (of_address_to_resource(atu_np, 0, &atu_res)) {
+		dev_err(vspadev->dev, "ATU reg map not found for %s%d\n",
+				VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	} else {
+		vspadev->vspa_atu_addr = (u32 __iomem *)atu_res.start;
+		vspadev->vspa_atu_size =  resource_size(&atu_res);
+	}
+
+	IF_DEBUG(DEBUG_STARTUP) {
+		for (win_cnt = 0; win_cnt < vspadev->num_atu_win; win_cnt++) {
+			pr_debug("ATU WIN[%d]START 0x%llx Size 0x%llx\n",
+					win_cnt,
+					vspadev->vspa_atu_win[win_cnt].windw_base_add,
+					vspadev->vspa_atu_win[win_cnt].windw_sz);
+		}
+	}
+
+	return 0;
+}
+
+static int
+vspa_getdts_properties(struct device_node *np, struct vspa_device *vspadev)
+{
+	u32 prop  = 0;
+	dma_addr_t prop1 = 0;
+	struct resource res;
+
+	if (!np)
+		return -EINVAL;
+
+	if (of_property_read_u32(np, "vspadev-id", &prop) < 0) {
+		dev_err(vspadev->dev, "vspadev-id attribute not found\n");
+		return -EINVAL;
+	}
+	vspadev->id = prop;
+
+	if (of_property_read_u64(np, "dbgregstart", &prop1) < 0) {
+		dev_err(vspadev->dev, "dbgregstart attribute not found for %s%d\n",
+				VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	}
+	vspadev->dbg_addr = (u32 *)(dma_addr_t)prop1;
+
+	if (of_property_read_u32(np, "dbgreglen", &prop) < 0) {
+		dev_err(vspadev->dev, "dbgreglen attribute not found for %s%d\n",
+				VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	}
+	vspadev->dbg_size = prop;
+
+	vspadev->cmd_buffer_bytes = cmd_buffer_bytes;
+	if (of_property_read_u32(np, "cmd_buf_size", &prop) >= 0)
+		vspadev->cmd_buffer_bytes = prop;
+	if (vspadev->cmd_buffer_bytes > (32*1024) ||
+		vspadev->cmd_buffer_bytes & 255) {
+		dev_err(vspadev->dev, "invalid cmd_buffer_size %d for %s%d\n",
+			vspadev->cmd_buffer_bytes, VSPA_DEVICE_NAME,
+			vspadev->id);
+		return -EINVAL;
+	}
+
+	vspadev->reply_buffer_bytes = reply_buffer_bytes;
+	if (of_property_read_u32(np, "reply_buf_size", &prop) >= 0)
+		vspadev->reply_buffer_bytes = prop;
+	if (vspadev->reply_buffer_bytes > (32*1024) ||
+		vspadev->reply_buffer_bytes & 255) {
+		dev_err(vspadev->dev, "invalid reply_buffer_size %d for %s%d\n",
+			vspadev->reply_buffer_bytes, VSPA_DEVICE_NAME,
+			vspadev->id);
+		return -EINVAL;
+	}
+
+	vspadev->spm_buffer_bytes = spm_buffer_bytes;
+	if (of_property_read_u32(np, "spm_buf_size", &prop) >= 0)
+		vspadev->spm_buffer_bytes = prop;
+	if (vspadev->spm_buffer_bytes > (32*1024) ||
+	    vspadev->spm_buffer_bytes & 255) {
+		dev_err(vspadev->dev, "invalid spm_buffer_size %d for %s%d\n",
+			vspadev->spm_buffer_bytes, VSPA_DEVICE_NAME,
+			vspadev->id);
+		return -EINVAL;
+	}
+
+	if (of_get_property(np, "interrupts", NULL)) {
+		vspadev->vspa_irq_no    = irq_of_parse_and_map(np, 0);
+		if (vspadev->vspa_irq_no == 0) {
+			dev_err(vspadev->dev, "Interrupt not found for %s%d\n",
+				 VSPA_DEVICE_NAME, vspadev->id);
+			return -EINVAL;
+		}
+	} else {
+		dev_err(vspadev->dev, "Interrupt numbers not found for %s%d\n",
+				 VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	}
+
+	if (of_address_to_resource(np, 0, &res)) {
+		dev_err(vspadev->dev, "VSPA reg map not found for %s%d\n",
+				VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	} else {
+		vspadev->mem_addr = (u32 __iomem *)res.start;
+		vspadev->mem_size =  resource_size(&res);
+	}
+
+	/* Parse ATU Node */
+	if (parse_atu_dts(np, vspadev)) {
+		dev_err(vspadev->dev, "VSPA-ATU DTS parsing failed %s%d\n",
+				VSPA_DEVICE_NAME, vspadev->id);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+void vspa_atu_enable(struct vspa_device *vspadev)
+{
+	int atu_win_cnt;
+
+	/* Enable all ATU windows configured from DTS */
+	for (atu_win_cnt = 0; atu_win_cnt < vspadev->num_atu_win; atu_win_cnt++) {
+
+		/* OWBAR provides windows that ATU maps inbound addresses to
+		[31 ................ 16 15 ........... 5 4 ........ 0 ]
+		  Window Base Address       Reserved      Window Size*/
+		vspa_reg_write(VSPA_ATU_OWBAR(atu_win_cnt),
+				(((u32)(u64) ATU_WINDW_BASE_ADD(atu_win_cnt) &
+				  (~SZ_TO_MASK(ATU_WINDW_SZ(atu_win_cnt)))) |
+				 ((ATU_WINDW_SZ(atu_win_cnt) - VSPA_ATU_MIN_WIN_SZ)
+				  & 0x1F)));
+
+		/* OTEAR provides bits [63:32] for the outbound ATU translation */
+		vspa_reg_write(VSPA_ATU_OTEAR(atu_win_cnt),
+				((ATU_WINDW_BASE_ADD(atu_win_cnt) &
+				  (-1LU << 32)) >> 32));
+
+		/* OTAR provides bits "31:31-n" for the outbound ATU translation
+		* where n is "16 - window size" */
+		vspa_reg_write(VSPA_ATU_OTAR(atu_win_cnt),
+				((u32)(u64)ATU_WINDW_BASE_ADD(atu_win_cnt) &
+				 (~SZ_TO_MASK(ATU_WINDW_SZ(atu_win_cnt)))));
+
+		/* OWAR enables the corresponding ATU translation windwos */
+		vspa_reg_write(VSPA_ATU_OWAR(atu_win_cnt),
+				0x80000000);
+	}
+}
+
+void vspa_atu_disable(struct vspa_device *vspadev)
+{
+	int atu_win_cnt;
+
+	/* Disable all ATU windows configured from DTS */
+	for (atu_win_cnt = 0; atu_win_cnt < vspadev->num_atu_win; atu_win_cnt++) {
+		vspa_reg_write(VSPA_ATU_OWAR(atu_win_cnt),
+				0x0);
+	}
+}
+
+static int vspa_probe(struct platform_device *pdev)
+{
+	struct vspa_device *vspadev = NULL;
+	struct device_node *np = pdev->dev.of_node;
+	struct device *dev = &pdev->dev;
+	struct vspa_hardware *hw;
+	struct device *sysfs_dev;
+	dev_t devno;
+	u8 device_name[10];
+	int err = 0;
+	dma_addr_t dma_paddr;
+	size_t dma_size;
+	uint32_t *dma_vaddr;
+	uint32_t param0, param1, param2;
+	uint32_t val;
+
+	BUG_ON(vspa_major == 0 || vspa_class == NULL);
+
+	/* Allocate vspa device structure */
+	vspadev = kzalloc(sizeof(struct vspa_device), GFP_KERNEL);
+	if (!vspadev) {
+		err = -ENOMEM;
+		goto err_out;
+	}
+
+	vspadev->dev = dev;
+
+	if (vspa_getdts_properties(np, vspadev) < 0) {
+		pr_info("VSPA DTS entry parse failed.\n");
+		err = -EINVAL;
+		goto err_struct;
+	}
+
+	devno = MKDEV(vspa_major, vspa_minor + vspadev->id);
+	sprintf(device_name, VSPA_DEVICE_NAME "%d", vspadev->id);
+
+	/* Setup DMA regions */
+	dma_size = vspadev->spm_buffer_bytes +
+		   vspadev->cmd_buffer_bytes +
+		   vspadev->reply_buffer_bytes;
+	dma_vaddr = dma_zalloc_coherent(dev, dma_size, &dma_paddr,
+					GFP_DMA);
+	if (dma_vaddr == NULL) {
+		pr_info("Failed to setup DMA buffer.\n");
+		err = -ENOMEM;
+		goto err_struct;
+	}
+
+	dev_set_drvdata(&pdev->dev, vspadev);
+	vspadev->state = VSPA_STATE_UNKNOWN;
+	vspadev->debug = DEBUG_MESSAGES;
+	vspadev->spm_buffer_paddr = &((uint32_t *)dma_paddr)[0];
+	vspadev->spm_buffer_vaddr = &dma_vaddr[0];
+	vspadev->spm_user_buf = 0;
+	cbuffer_init(&vspadev->cmd_buffer,
+			vspadev->cmd_buffer_bytes >> 2,
+			&((uint32_t *)dma_paddr)
+			[vspadev->spm_buffer_bytes >> 2],
+			&dma_vaddr[vspadev->spm_buffer_bytes>>2]);
+	pool_init(&vspadev->cmd_pool, 256, 0, 0);
+	pool_init(&vspadev->reply_pool,
+			vspadev->reply_buffer_bytes >> 2,
+			&((uint32_t *)dma_paddr)
+			[(vspadev->spm_buffer_bytes >> 2) +
+			(vspadev->cmd_buffer_bytes >> 2)],
+			&dma_vaddr[(vspadev->spm_buffer_bytes >> 2) +
+			(vspadev->cmd_buffer_bytes >> 2)]);
+
+	/* Map VSPA registers */
+	vspadev->regs = ioremap((dma_addr_t)vspadev->mem_addr,
+			vspadev->mem_size);
+	vspadev->dbg_regs = ioremap((dma_addr_t)vspadev->dbg_addr,
+			vspadev->dbg_size);
+	/* Map VSPA-ATU registers */
+	vspadev->vspa_atu_regs = ioremap((dma_addr_t)vspadev->vspa_atu_addr,
+			vspadev->vspa_atu_size);
+
+	hw = &vspadev->hardware;
+	param0 = vspa_reg_read(vspadev->regs + PARAM0_REG_OFFSET);
+	hw->param0           = param0;
+	param1 = vspa_reg_read(vspadev->regs + PARAM1_REG_OFFSET);
+	hw->param1           = param1;
+	hw->axi_data_width   = 32 << ((param1 >> 28) & 7);
+	hw->dma_channels     = (param1 >> 16) & 0xFF;
+	hw->gp_out_regs      = (param1 >> 8) & 0xFF;
+	hw->gp_in_regs       = param1 & 0xFF;
+	param2 = vspa_reg_read(vspadev->regs + PARAM2_REG_OFFSET);
+	hw->param2           = param2;
+	hw->dmem_bytes       = ((param2 >> 8) & 0x3FF) * 400;
+	hw->ippu_bytes       = (param2 >> 31) * 4096;
+	hw->arithmetic_units = param2 & 0xFF;
+
+	vspadev->versions.vspa_hw_version =
+			vspa_reg_read(vspadev->regs+HWVERSION_REG_OFFSET);
+	vspadev->versions.ippu_hw_version =
+			vspa_reg_read(vspadev->regs+IPPU_HWVERSION_REG_OFFSET);
+	vspadev->versions.vspa_sw_version = ~0;
+	vspadev->versions.ippu_sw_version = ~0;
+	vspadev->eld_filename[0] = '\0';
+	vspadev->watchdog_interval_msecs = VSPA_WATCHDOG_INTERVAL_DEFAULT;
+	init_completion(&vspadev->watchdog_complete);
+	setup_timer(&vspadev->watchdog_timer, watchdog_callback,
+						(unsigned long)vspadev);
+	init_completion(&vspadev->mbchk_complete);
+	setup_timer(&vspadev->mbchk_timer, mbchk_callback,
+			(unsigned long)vspadev);
+
+	spin_lock_init(&vspadev->dma_tx_queue_lock);
+	spin_lock_init(&vspadev->dma_enqueue_lock);
+	spin_lock_init(&vspadev->event_list_lock);
+	spin_lock_init(&vspadev->event_queue_lock);
+	spin_lock_init(&vspadev->mb64_lock);
+	spin_lock_init(&vspadev->mbchk_lock);
+	spin_lock_init(&vspadev->control_lock);
+	vspadev->poll_mask = VSPA_MSG_ALL;
+	/* spin_lock_init(&vspadev->irq_lock); */
+	init_waitqueue_head(&(vspadev->event_wait_q));
+	event_reset_queue(vspadev);
+	dma_reset_queue(vspadev);
+	mbox_reset(vspadev);
+	cmd_reset(vspadev);
+
+	/* Enable core power gating */
+	val = vspa_reg_read(vspadev->regs + CONTROL_REG_OFFSET);
+	val = (val & CONTROL_REG_MASK) | CONTROL_PDN_EN;
+	vspa_reg_write(vspadev->regs + CONTROL_REG_OFFSET, val);
+
+	err = request_irq(vspadev->vspa_irq_no, vspa_irq_handler,
+				0, device_name, vspadev);
+	if (err < 0) {
+		pr_info("%s: request_irq() err = %d\n",
+			device_name, err);
+		goto err_ioremap;
+	}
+	cdev_init(&vspadev->cdev, &vspa_fops);
+	vspadev->cdev.owner = THIS_MODULE;
+
+	err = cdev_add(&vspadev->cdev, devno, 1);
+	if (err < 0) {
+		pr_info("Error %d while adding %s", err, device_name);
+		goto err_add;
+	}
+
+	/* Create sysfs device */
+	sysfs_dev = device_create(vspa_class, dev, devno, NULL, device_name);
+	if (IS_ERR(sysfs_dev)) {
+		err = PTR_ERR(sysfs_dev);
+		pr_info("Error %d while creating %s", err, device_name);
+		goto err_create;
+	}
+
+	err = sysfs_create_group(&dev->kobj, &attr_group);
+	if (err < 0) {
+		pr_info("error %d while creating group %s", err,
+								device_name);
+		goto err_group;
+	}
+
+	/* Make sure all interrupts are disabled */
+	vspa_reg_write(vspadev->regs + IRQEN_REG_OFFSET, 0);
+	pr_info("%s: hwver 0x%08x, %d AUs, dmem %d bytes\n",
+			device_name, vspadev->regs[HWVERSION_REG_OFFSET],
+			hw->arithmetic_units, hw->dmem_bytes);
+
+	/* Enable ATU entries */
+	/* TODO: Can this be handled using ioctl()
+	 * as ATU is not part of VSPA device */
+	if (0 == vspa_devs)
+		vspa_atu_enable(vspadev);
+
+	vspa_devs++;
+	return 0;
+
+err_group:
+	device_destroy(vspa_class, devno);
+err_create:
+	cdev_del(&vspadev->cdev);
+err_add:
+	free_irq(vspadev->vspa_irq_no, vspadev);
+err_ioremap:
+	iounmap(vspadev->regs);
+	iounmap(vspadev->dbg_regs);
+	vspa_atu_disable(vspadev);
+	iounmap(vspadev->vspa_atu_regs);
+	dma_free_coherent(dev, dma_size, dma_vaddr, dma_paddr);
+err_struct:
+	kfree(vspadev);
+err_out:
+	return err;
+}
+
+static int vspa_remove(struct platform_device *ofpdev)
+{
+	struct device *dev = &ofpdev->dev;
+	struct vspa_device *vspadev = dev_get_drvdata(&ofpdev->dev);
+
+	BUG_ON(vspadev == NULL || vspa_class == NULL);
+
+	/* shutdown timer cleanly */
+	vspadev->watchdog_interval_msecs = 0;
+	init_completion(&vspadev->mbchk_complete);
+	init_completion(&vspadev->watchdog_complete);
+	mod_timer(&vspadev->mbchk_timer, jiffies);
+	mod_timer(&vspadev->watchdog_timer, jiffies);
+	wait_for_completion(&vspadev->mbchk_complete);
+	wait_for_completion(&vspadev->watchdog_complete);
+	del_timer_sync(&vspadev->mbchk_timer);
+	del_timer_sync(&vspadev->watchdog_timer);
+
+	/* Disbale all irqs of VSPA */
+	vspa_reg_write(vspadev->regs + IRQEN_REG_OFFSET, 0);
+	free_irq(vspadev->vspa_irq_no, vspadev);
+
+	if (vspadev->spm_user_buf) {
+		vspadev->spm_user_buf = 0;
+		iounmap(vspadev->spm_buf_vaddr);
+	}
+
+	dev_set_drvdata(&ofpdev->dev, NULL);
+
+	iounmap(vspadev->regs);
+	iounmap(vspadev->dbg_regs);
+	vspa_atu_disable(vspadev);
+	iounmap(vspadev->vspa_atu_regs);
+
+	sysfs_remove_group(&dev->kobj, &attr_group);
+	device_destroy(vspa_class, MKDEV(vspa_major, vspa_minor + vspadev->id));
+	cdev_del(&vspadev->cdev);
+	kfree(vspadev);
+
+	if (vspa_devs > 0)
+		vspa_devs--;
+
+	return 0;
+}
+
+static const struct of_device_id vspa_match[] = {
+	{.compatible = "fsl,vspa",},
+	{},
+};
+
+static struct platform_driver vspa_driver = {
+	.driver = {
+		.name = "fsl-vspa",
+		.owner = THIS_MODULE,
+		.of_match_table = vspa_match,
+	},
+	.probe = vspa_probe,
+	.remove = vspa_remove,
+};
+
+static int __init vspa_mod_init(void)
+{
+	int err;
+	dev_t devno;
+
+	/* Register our major, and accept a dynamic number. */
+	err = alloc_chrdev_region(&devno, 0, MAX_VSPA, VSPA_DEVICE_NAME);
+	if (err < 0) {
+		pr_err("vspa: can't get major number: %d\n", err);
+		goto chrdev_fail;
+	}
+	vspa_major = MAJOR(devno);
+	vspa_minor = MINOR(devno);
+
+	/* Create the device class if required */
+	vspa_class = class_create(THIS_MODULE, VSPA_DEVICE_NAME);
+	if (IS_ERR(vspa_class)) {
+		err = PTR_ERR(vspa_class);
+		pr_err("vspa: class_create() failed %d\n", err);
+		goto class_fail;
+	}
+
+	err = platform_driver_register(&vspa_driver);
+	if (err == 0)
+		return 0;
+
+	class_destroy(vspa_class);
+class_fail:
+	unregister_chrdev_region(vspa_major, MAX_VSPA);
+chrdev_fail:
+	return err;
+}
+
+static void __exit vspa_exit(void)
+{
+	platform_driver_unregister(&vspa_driver);
+	class_destroy(vspa_class);
+	unregister_chrdev_region(vspa_major, MAX_VSPA);
+}
+
+module_init(vspa_mod_init);
+module_exit(vspa_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Freescale Semiconductor, Inc.");
+MODULE_DESCRIPTION("Freescale VSPA Driver");
diff --git a/drivers/misc/vspa/sys.c b/drivers/misc/vspa/sys.c
new file mode 100644
index 0000000..54061a5
--- /dev/null
+++ b/drivers/misc/vspa/sys.c
@@ -0,0 +1,230 @@
+/*
+ * drivers/misc/vspa.c
+ * VSPA device driver
+ *
+ * Copyright (C) 2016 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/types.h>
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/list.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/wait.h>
+#include <linux/uaccess.h>
+#include <linux/signal.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/input.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_gpio.h>
+#include <linux/of_address.h>
+#include <linux/pid.h>
+#include <linux/mm.h>
+#include <linux/poll.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <linux/list.h>
+
+#include <uapi/linux/vspa.h>
+#include "vspa.h"
+
+/*
+ * Sysfs
+ * TODO - complete sysfs: registers, bds
+ */
+
+static const char *event_name[16] = {
+	"?00?", "DMA", "CMD", "REPLY", "SPM", "MB64I", "MB32I", "MB64O",
+	"MB32O", "ERR", "?10?", "?11?", "?12?", "?13?", "?14?", "?15?"
+};
+
+static const char * const state_name[] = {
+	"unknown", "powerdown", "unprogrammed_idle", "unprogrammed_busy",
+	"loading", "startup_error", "running_idle", "running_busy"
+};
+
+static ssize_t show_stat(struct device *, struct device_attribute *, char *);
+static ssize_t show_text(struct device *, struct device_attribute *, char *);
+
+static ssize_t set_debug(struct device *dev, struct device_attribute *devattr,
+			const char *buf, size_t count)
+{
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+	int err;
+	unsigned int val;
+
+	err = kstrtouint(buf, 0, &val);
+	if (err)
+		return err;
+
+	vspadev->debug = val;
+	return count;
+}
+
+static ssize_t show_debug(struct device *dev,
+		struct device_attribute *devattr, char *buf)
+{
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+
+	return sprintf(buf, "0x%08X\n", vspadev->debug);
+}
+
+static ssize_t show_versions(struct device *dev,
+		struct device_attribute *devattr, char *buf)
+{
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+
+	return sprintf(buf, "0x%08X 0x%08X 0x%08X 0x%08X\n",
+			vspadev->versions.vspa_hw_version,
+			vspadev->versions.ippu_hw_version,
+			vspadev->versions.vspa_sw_version,
+			vspadev->versions.ippu_sw_version);
+}
+
+static ssize_t show_events(struct device *dev,
+		struct device_attribute *devattr, char *buf)
+{
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+	struct event_list *ptr;
+	size_t ctr, len, total_len;
+
+	event_list_update(vspadev);
+
+	spin_lock(&vspadev->event_list_lock);
+	ctr = 0;
+	total_len = 0;
+	list_for_each_entry(ptr, &vspadev->events_queued_list, list) {
+		len = sprintf(buf, "[%2zu] %-5s %02X %02X %08X %08X\n",
+			ctr++, event_name[ptr->type & 0xF], ptr->id,
+			ptr->err, ptr->data0, ptr->data1);
+		total_len += len;
+		buf += len;
+		if (total_len >= (PAGE_SIZE - 50)) {
+			len = sprintf(buf, "...\n");
+			total_len += len;
+			break;
+		}
+	}
+	spin_unlock(&vspadev->event_list_lock);
+	return total_len;
+}
+
+static ssize_t show_seqids(struct device *dev,
+		struct device_attribute *devattr, char *buf)
+{
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+	struct seqid *s;
+	int i;
+	size_t len, total_len;
+
+	spin_lock(&vspadev->control_lock);
+	total_len = 0;
+	for (i = 0; i < MAX_SEQIDS; i++) {
+		if (vspadev->active_seqids & (1<<i)) {
+			s = &(vspadev->seqid[i]);
+			len = sprintf(buf, "[%2d] %02X %04X %2d %4d %4d %2d %4d %08X\n",
+					i, s->cmd_id, s->flags, s->cmd_bd_index,
+					s->cmd_buffer_idx, s->cmd_buffer_size,
+					s->reply_bd_index, s->reply_size,
+					s->payload1);
+			total_len += len;
+			buf += len;
+			if (total_len >= (PAGE_SIZE - 80)) {
+				len = sprintf(buf, "...\n");
+				total_len += len;
+				break;
+			}
+		}
+	}
+	spin_unlock(&vspadev->control_lock);
+	return total_len;
+}
+
+static DEVICE_ATTR(debug,  S_IWUSR | S_IRUGO, show_debug,    set_debug);
+static DEVICE_ATTR(eld_filename,     S_IRUGO, show_text,     NULL);
+static DEVICE_ATTR(events,           S_IRUGO, show_events,   NULL);
+static DEVICE_ATTR(seqids,           S_IRUGO, show_seqids,   NULL);
+static DEVICE_ATTR(state,            S_IRUGO, show_stat,     NULL);
+static DEVICE_ATTR(state_name,       S_IRUGO, show_text,     NULL);
+static DEVICE_ATTR(versions,         S_IRUGO, show_versions, NULL);
+
+static struct attribute *attributes[] = {
+	&dev_attr_debug.attr,
+	&dev_attr_eld_filename.attr,
+	&dev_attr_events.attr,
+	&dev_attr_seqids.attr,
+	&dev_attr_state.attr,
+	&dev_attr_state_name.attr,
+	&dev_attr_versions.attr,
+	NULL
+};
+
+const struct attribute_group attr_group = {
+	.attrs = attributes,
+};
+
+static ssize_t show_stat(struct device *dev,
+			struct device_attribute *devattr, char *buf)
+{
+	unsigned int val;
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+
+	if (devattr == &dev_attr_state)
+		val = full_state(vspadev);
+	else
+		val = 0;
+
+	return sprintf(buf, "%d\n", val);
+}
+
+static ssize_t show_text(struct device *dev,
+			struct device_attribute *devattr, char *buf)
+{
+	const char *ptr;
+	struct vspa_device *vspadev = dev_get_drvdata(dev);
+
+	if (devattr == &dev_attr_eld_filename)
+		ptr = vspadev->eld_filename;
+	else
+		if (devattr == &dev_attr_state_name)
+			ptr = state_name[full_state(vspadev)];
+		else
+			ptr = "??";
+
+	return sprintf(buf, "%s\n", ptr == NULL ? "NULL" : ptr);
+}
diff --git a/drivers/misc/vspa/vspa.h b/drivers/misc/vspa/vspa.h
new file mode 100644
index 0000000..dce956c
--- /dev/null
+++ b/drivers/misc/vspa/vspa.h
@@ -0,0 +1,426 @@
+/*
+ * Copyright (C) 2016 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __VSPA_H_
+#define __VSPA_H_
+
+#include <linux/cdev.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/wait.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/completion.h>
+#include <uapi/linux/vspa.h>
+
+/* Debug and error reporting macros */
+#define IF_DEBUG(x)	if (vspadev->debug & (x))
+#define ERR(...)	{ if (vspadev->debug & DEBUG_MESSAGES) \
+				pr_err(VSPA_DEVICE_NAME __VA_ARGS__); }
+#define MAX_VSPA 4
+#define VSPA_DEVICE_NAME "vspa"
+
+/* Debug bit assignments */
+#define DEBUG_MESSAGES			(1<<0)
+#define DEBUG_STARTUP			(1<<1)
+#define DEBUG_CMD			(1<<2)
+#define DEBUG_REPLY			(1<<3)
+#define DEBUG_SPM			(1<<4)
+#define DEBUG_DMA			(1<<5)
+#define DEBUG_EVENT			(1<<6)
+#define DEBUG_WATCHDOG			(1<<7)
+#define DEBUG_MBOX64_OUT		(1<<8)
+#define DEBUG_MBOX32_OUT		(1<<9)
+#define DEBUG_MBOX64_IN			(1<<10)
+#define DEBUG_MBOX32_IN			(1<<11)
+#define DEBUG_DMA_IRQ			(1<<12)
+#define DEBUG_FLAGS0_IRQ		(1<<13)
+#define DEBUG_FLAGS1_IRQ		(1<<14)
+#define DEBUG_IOCTL			(1<<15)
+#define DEBUG_SEQID			(1<<16)
+#define DEBUG_CMD_BD			(1<<17)
+#define DEBUG_REPLY_BD			(1<<18)
+#define DEBUG_TEST_SPM			(1<<24)
+
+/* IP register offset for the registers used by the driver */
+#define HWVERSION_REG_OFFSET		(0x0000>>2)
+#define SWVERSION_REG_OFFSET		(0x0004>>2)
+#define CONTROL_REG_OFFSET		(0x0008>>2)
+#define IRQEN_REG_OFFSET		(0x000C>>2)
+#define STATUS_REG_OFFSET		(0x0010>>2)
+#define HOST_FLAGS0_REG_OFFSET		(0x0014>>2)
+#define HOST_FLAGS1_REG_OFFSET		(0x0018>>2)
+#define VCPU_FLAGS0_REG_OFFSET		(0x001C>>2)
+#define VCPU_FLAGS1_REG_OFFSET		(0x0020>>2)
+#define EXT_GO_ENABLE_REG_OFFSET	(0x0028>>2)
+#define EXT_GO_STATUS_REG_OFFSET	(0x002C>>2)
+#define PARAM0_REG_OFFSET		(0x0040>>2)
+#define PARAM1_REG_OFFSET		(0x0044>>2)
+#define PARAM2_REG_OFFSET		(0x0048>>2)
+#define DMA_DMEM_ADDR_REG_OFFSET	(0x00B0>>2)
+#define DMA_AXI_ADDR_REG_OFFSET		(0x00B4>>2)
+#define DMA_BYTE_CNT_REG_OFFSET		(0x00B8>>2)
+#define DMA_XFR_CTRL_REG_OFFSET		(0x00BC>>2)
+#define DMA_STAT_ABORT_REG_OFFSET	(0x00C0>>2)
+#define DMA_IRQ_STAT_REG_OFFSET		(0x00C4>>2)
+#define DMA_COMP_STAT_REG_OFFSET	(0x00C8>>2)
+#define DMA_XFRERR_STAT_REG_OFFSET	(0x00CC>>2)
+#define DMA_CFGERR_STAT_REG_OFFSET	(0x00D0>>2)
+#define DMA_XRUN_STAT_REG_OFFSET	(0x00D4>>2)
+#define DMA_GO_STAT_REG_OFFSET		(0x00D8>>2)
+#define DMA_FIFO_STAT_REG_OFFSET	(0x00DC>>2)
+
+#define GP_OUT1_REG_OFFSET		(0x0580>>2)
+#define GP_OUT2_REG_OFFSET		(0x0584>>2)
+#define GP_OUT3_REG_OFFSET		(0x0588>>2)
+
+#define HOST_OUT_64_MSB_REG_OFFSET	(0x0680>>2)
+#define HOST_OUT_64_LSB_REG_OFFSET	(0x0684>>2)
+#define HOST_OUT_1_64_MSB_REG_OFFSET	(0x0688>>2)
+#define HOST_OUT_1_64_LSB_REG_OFFSET	(0x068C>>2)
+#define HOST_IN_64_MSB_REG_OFFSET	(0x0690>>2)
+#define HOST_IN_64_LSB_REG_OFFSET	(0x0694>>2)
+#define HOST_IN_1_64_MSB_REG_OFFSET	(0x0698>>2)
+#define HOST_IN_1_64_LSB_REG_OFFSET	(0x069C>>2)
+#define HOST_MBOX_STATUS_REG_OFFSET     (0x06A0>>2)
+
+#define IPPU_CONTROL_REG_OFFSET		(0x0700>>2)
+#define IPPU_STATUS_REG_OFFSET		(0x0704>>2)
+#define IPPU_ARG_BASEADDR_REG_OFFSET	(0x070C>>2)
+#define IPPU_HWVERSION_REG_OFFSET	(0x0710>>2)
+#define IPPU_SWVERSION_REG_OFFSET	(0x0714>>2)
+
+#define DBG_GDBEN_REG_OFFSET		(0x800>>2)
+#define DBG_RCR_REG_OFFSET		(0x804>>2)
+#define DBG_RCSTATUS_REG_OFFSET		(0x808>>2)
+#define DBG_RAVAP_REG_OFFSET		(0x870>>2)
+#define DBG_DVR_REG_OFFSET		(0x87C>>2)
+
+
+#define STATUS_REG_PDN_ACTIVE		(0x80000000)
+#define STATUS_REG_PDN_DONE		(0x40000000)
+#define STATUS_REG_IRQ_VCPU_READ_MSG	(0x0000C000)
+#define STATUS_REG_IRQ_VCPU_SENT_MSG	(0x00003000)
+#define STATUS_REG_IRQ_VCPU_MSG		(STATUS_REG_IRQ_VCPU_READ_MSG | \
+					STATUS_REG_IRQ_VCPU_SENT_MSG)
+#define STATUS_REG_BUSY			(0x00000100)
+#define STATUS_REG_IRQ_DMA_ERR		(0x00000020)
+#define STATUS_REG_IRQ_DMA_COMP		(0x00000010)
+#define STATUS_REG_IRQ_FLAGS1		(0x00000008)
+#define STATUS_REG_IRQ_FLAGS0		(0x00000004)
+#define STATUS_REG_IRQ_IPPU_DONE	(0x00000002)
+#define STATUS_REG_IRQ_DONE		(0x00000001)
+#define STATUS_REG_IRQ_NON_GEN         (STATUS_REG_IRQ_FLAGS1 |		  \
+			STATUS_REG_IRQ_FLAGS0 | STATUS_REG_IRQ_DMA_COMP | \
+			STATUS_REG_IRQ_DMA_ERR | STATUS_REG_IRQ_VCPU_SENT_MSG)
+
+#define MBOX_STATUS_IN_1_64_BIT		(0x00000008)
+#define MBOX_STATUS_IN_64_BIT		(0x00000004)
+#define MBOX_STATUS_OUT_1_64_BIT	(0x00000002)
+#define MBOX_STATUS_OUT_64_BIT		(0x00000001)
+
+#define VSPA_DMA_CHANNELS		(32)
+
+/* mmap offset argument for vspa regsiters */
+#define VSPA_REG_OFFSET			(0)
+
+/* mmap offset argument for dbg regsiter */
+#define VSPA_DBG_OFFSET			(4096)
+
+#define DMA_FLAG_COMPLETE		(1<<0)
+#define DMA_FLAG_XFRERR			(1<<1)
+#define DMA_FLAG_CFGERR			(1<<2)
+
+#define MAX_VSPA_ATU_WIN		8
+#define VSPA_ATU_OWBAR(x)		(vspadev->vspa_atu_regs + \
+								(0x110U >> 2) + \
+								0x4*(x))
+#define VSPA_ATU_OWAR(x)		(vspadev->vspa_atu_regs + \
+								(0x114U >> 2) + \
+								0x4*(x))
+#define VSPA_ATU_OTEAR(x)		(vspadev->vspa_atu_regs + \
+								(0x118U >> 2) + \
+								0x4*(x))
+#define VSPA_ATU_OTAR(x)		(vspadev->vspa_atu_regs + \
+								(0x11CU >> 2) + \
+								0x4*(x))
+
+#define MBOX_QUEUE_ENTRIES		(16)
+struct mbox_queue {
+	int	idx_enqueue;
+	int	idx_dequeue;
+	int	idx_complete;
+};
+
+#define EVENT_LIST_ENTRIES		(256)
+struct event_list {
+	struct list_head list;
+	union {
+	  uint32_t	control;
+	  struct {
+	    uint8_t	id;
+	    uint8_t	err;
+	    uint16_t	type;
+	  };
+	};
+	uint32_t	data0;
+	uint32_t	data1;
+};
+
+struct event_entry {
+	union {
+	  uint32_t      control;
+	  struct {
+	    uint8_t     id;
+	    uint8_t     err;
+	    uint8_t     rsvd;
+	    uint8_t     type;
+	  };
+	};
+	uint32_t	data0;
+	uint32_t	data1;
+	uint32_t	lost;
+};
+
+#define EVENT_QUEUE_ENTRIES		(256)
+struct event_queue {
+	struct event_entry entry[EVENT_QUEUE_ENTRIES];
+	int    idx_enqueue;	/* Index for next item to enqueue */
+	int    idx_queued;	/* Index for last item enqueued */
+	int    idx_dequeue;	/* Index for next item to dequeue */
+};
+
+#define DMA_QUEUE_ENTRIES		(16)
+struct dma_queue {
+	struct vspa_dma_req entry[DMA_QUEUE_ENTRIES];
+	int		chan;
+	int		idx_queue;	/* Index for next item to enqueue */
+	int		idx_dma;	/* Index for next item to DMA */
+	int		idx_chk;	/* Index for next item to check */
+};
+
+struct memory_pool_bd {
+	uint32_t	start;
+	uint32_t	size;
+	uint32_t	free;
+	uint32_t	wstart;
+	int8_t		next;
+};
+
+/*TODO - support 32 reply bds */
+#define BD_ENTRIES		(16)
+struct memory_pool {
+	uint32_t	free_bds;
+	struct memory_pool_bd bd[BD_ENTRIES];
+	uint32_t	tail;
+	uint32_t	size;
+	uint32_t	*paddr;
+	uint32_t	*vaddr;
+};
+
+struct circular_buffer {
+	uint32_t	write_idx;
+	uint32_t	read_idx;
+	uint32_t	size;
+	uint32_t	used;
+	uint32_t	*paddr;
+	uint32_t	*vaddr;
+};
+
+#define MAX_SEQIDS			(16)
+struct seqid {
+	uint32_t	flags;
+	int		cmd_id;
+	int		cmd_buffer_idx;
+	int		cmd_buffer_size;
+	int		cmd_bd_index;
+	int		reply_bd_index;
+	int		reply_size;
+	uint32_t	payload1;
+};
+
+struct atu_window {
+	uint64_t windw_base_add;
+	uint64_t windw_sz;
+};
+
+/* The below structure contains all the information for the
+* vspa device required by the kernel driver
+*/
+struct vspa_device {
+
+	/* VSPA instance */
+	int		id;
+
+	struct device	*dev;
+
+	/* Char device structure */
+	struct cdev	cdev;
+
+	/* Major minor information */
+	dev_t		dev_t;
+
+	/* Current state of the device */
+	enum vspa_state	state;
+	uint32_t	debug;
+
+	/* IRQ numbers */
+	uint32_t	vspa_irq_no;
+	/* IP registers */
+	resource_size_t	mem_size; /* size */
+	u32 __iomem	*mem_addr;    /* physical address */
+	u32 __iomem	*regs;	/* virtual address */
+
+	/* TODO: ATU is not part of VSPA device
+	 * keeping ATU data structure in VSPA device fior now */
+	u32 __iomem	*vspa_atu_regs; /* Virtual address */
+	u32 __iomem	*vspa_atu_addr; /* Physical address */
+	uint32_t	vspa_atu_size;
+	uint32_t num_atu_win;
+	struct atu_window vspa_atu_win[MAX_VSPA_ATU_WIN];
+
+	/* Debug registers */
+	resource_size_t	dbg_size; /* size */
+	u32 __iomem	*dbg_addr;    /* physical address */
+	u32 __iomem	*dbg_regs;    /* virtual address */
+
+	/* Buffer sizes */
+	uint32_t	spm_buffer_bytes;
+	uint32_t	cmd_buffer_bytes;
+	uint32_t	reply_buffer_bytes;
+	uint32_t	*spm_buffer_paddr;
+	uint32_t	*spm_buffer_vaddr;
+
+	/* Working set for SPM buffer */
+	int		spm_user_buf;
+	uint32_t	spm_buf_bytes;
+	uint32_t	*spm_buf_paddr;
+	uint32_t	*spm_buf_vaddr;
+
+	/* DMA queue */
+	struct dma_queue dma_queue;
+	spinlock_t	dma_enqueue_lock;
+	spinlock_t	dma_tx_queue_lock; /* called from irq handler */
+	unsigned long	dma_tx_queue_lock_flags;
+
+	/* Event queue */
+	struct event_queue event_queue;
+	struct event_list events[EVENT_LIST_ENTRIES];
+	struct list_head events_free_list;
+	struct list_head events_queued_list;
+	spinlock_t	event_list_lock;
+	spinlock_t	event_queue_lock; /* called from irq handler */
+
+	/* Memory pools */
+	struct circular_buffer cmd_buffer;
+	struct memory_pool cmd_pool;
+	struct memory_pool reply_pool;
+
+	uint32_t	cmdbuf_addr;
+	uint32_t	spm_addr;
+
+	/* Sequence IDs */
+	uint32_t	active_seqids;
+	uint32_t	last_seqid;
+	struct seqid	seqid[MAX_SEQIDS];
+
+	uint32_t	first_cmd;
+	uint32_t	last_wstart;
+
+	struct mbox_queue mb32_queue;
+	struct mbox_queue mb64_queue;
+	struct vspa_mb32 mb32[MBOX_QUEUE_ENTRIES];
+	struct vspa_mb64 mb64[MBOX_QUEUE_ENTRIES];
+	spinlock_t	mb32_lock; /* called from irq handler */
+	spinlock_t	mb64_lock; /* called from irq handler */
+	spinlock_t	mbchk_lock; /* called from irq handler */
+	struct timer_list mbchk_timer;
+	struct completion mbchk_complete;
+
+	/* DMA channel usage */
+	uint8_t		spm_dma_chan;
+	uint8_t		bulk_dma_chan;
+	uint8_t		reply_dma_chan;
+	uint8_t		cmd_dma_chan;
+
+	uint8_t		legacy_cmd_dma_chan;
+	char		eld_filename[VSPA_MAX_ELD_FILENAME];
+	struct vspa_versions versions;
+	struct vspa_hardware hardware;
+
+	/* Watchdog */
+	struct timer_list watchdog_timer;
+	uint32_t	watchdog_interval_msecs;
+	uint32_t	watchdog_value;
+	struct completion watchdog_complete;
+
+	/* IRQ handling */
+/* TODO	spinlock_t irq_lock; */
+	uint32_t	irq_bits; /* TODO - test code */
+
+	spinlock_t	control_lock;
+
+	/* Wait queue for event notifications*/
+	wait_queue_head_t event_wait_q;
+	uint32_t	event_list_mask;
+	uint32_t	event_queue_mask;
+	uint32_t	poll_mask;
+};
+
+extern const struct attribute_group attr_group;
+
+/*
+ * Accessor Functions
+ */
+
+static inline void vspa_reg_write(void __iomem *addr, u32 val)
+{
+	return iowrite32(val, addr);
+}
+
+static inline unsigned int vspa_reg_read(void __iomem *addr)
+{
+	return ioread32(addr);
+}
+
+int full_state(struct vspa_device *vspadev);
+void pool_free_bd(struct memory_pool *pool, uint8_t index);
+
+void event_reset_queue(struct vspa_device *vspadev);
+void event_enqueue(struct vspa_device *vspadev, uint8_t type,
+	uint8_t id, uint8_t err, uint32_t data0, uint32_t data1);
+void event_list_update(struct vspa_device *vspadev);
+int read_event(struct vspa_device *vspadev,
+	struct vspa_event_read *evt_rd);
+u32 vspa_map_axi_addr_atu_win(struct vspa_device *, dma_addr_t);
+
+#endif /* _VSPA_H */
diff --git a/include/uapi/linux/vspa.h b/include/uapi/linux/vspa.h
new file mode 100644
index 0000000..74a912f
--- /dev/null
+++ b/include/uapi/linux/vspa.h
@@ -0,0 +1,241 @@
+/*
+ * Copyright (C) 2016 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _UAPI_VSPA_H
+#define _UAPI_VSPA_H
+
+#define VSPA_MAGIC_NUM 'V'
+
+/* Maximum allowed size for any individual command */
+#define CMD_MAX_SZ_BYTES	(128)
+
+enum vspa_state {
+	VSPA_STATE_UNKNOWN = 0,
+	VSPA_STATE_POWER_DOWN,
+	VSPA_STATE_STARTUP_ERR,
+	VSPA_STATE_UNPROGRAMMED_IDLE,
+	VSPA_STATE_UNPROGRAMMED_BUSY,
+	VSPA_STATE_LOADING,
+	VSPA_STATE_RUNNING_IDLE,
+	VSPA_STATE_RUNNING_BUSY
+};
+
+enum vspa_power {
+	VSPA_POWER_DOWN = 0,
+	VSPA_POWER_UP,
+	VSPA_POWER_CYCLE
+};
+
+enum vspa_event_type {
+	VSPA_EVENT_NONE = 0,
+	VSPA_EVENT_DMA,
+	VSPA_EVENT_CMD,
+	VSPA_EVENT_REPLY,
+	VSPA_EVENT_SPM,
+	VSPA_EVENT_MB64_IN,
+	VSPA_EVENT_MB32_IN,
+	VSPA_EVENT_MB64_OUT,
+	VSPA_EVENT_MB32_OUT,
+	VSPA_EVENT_ERROR
+};
+
+#define VSPA_FLAG_EXPECT_CMD_REPLY	(1<<0)
+#define VSPA_FLAG_REPORT_CMD_REPLY	(1<<1)
+#define VSPA_FLAG_REPORT_CMD_CONSUMED	(1<<2)
+#define VSPA_FLAG_REPORT_DMA_COMPLETE	(1<<3)
+#define VSPA_FLAG_REPORT_MB_COMPLETE	(1<<4)
+
+
+#define VSPA_MSG(x)			(0x10 << x)
+
+#define VSPA_MSG_PEEK			VSPA_MSG(VSPA_EVENT_NONE)
+#define VSPA_MSG_DMA			VSPA_MSG(VSPA_EVENT_DMA)
+#define VSPA_MSG_CMD			VSPA_MSG(VSPA_EVENT_CMD)
+#define VSPA_MSG_REPLY			VSPA_MSG(VSPA_EVENT_REPLY)
+#define VSPA_MSG_SPM			VSPA_MSG(VSPA_EVENT_SPM)
+#define VSPA_MSG_MB64_IN		VSPA_MSG(VSPA_EVENT_MB64_IN)
+#define VSPA_MSG_MB32_IN		VSPA_MSG(VSPA_EVENT_MB32_IN)
+#define VSPA_MSG_MB64_OUT		VSPA_MSG(VSPA_EVENT_MB64_OUT)
+#define VSPA_MSG_MB32_OUT		VSPA_MSG(VSPA_EVENT_MB32_OUT)
+#define VSPA_MSG_ERROR			VSPA_MSG(VSPA_EVENT_ERROR)
+#define VSPA_MSG_ALL			(0xFFF0)
+#define VSPA_MSG_ALL_EVENTS		(0xFFE0)
+
+#define VSPA_ERR_DMA_CFGERR		(0x10)
+#define VSPA_ERR_DMA_XFRERR		(0x11)
+#define VSPA_ERR_WATCHDOG		(0x12)
+
+struct vspa_event {
+	union {
+	  uint32_t	control;
+	  struct {
+	    uint8_t	id;
+	    uint8_t	err;
+	    uint16_t	type;
+	  };
+	};
+	uint16_t	pkt_size;
+	uint16_t	buf_size;
+	uint32_t	data[0];
+};
+
+struct vspa_dma_req {
+	union {
+	  uint32_t	control;
+	  struct {
+	    uint8_t	id;
+	    uint8_t	flags;
+	    uint8_t	rsvd;
+	    uint8_t	type;
+	  };
+	};
+	uint32_t	dmem_addr;
+	dma_addr_t	axi_addr;
+	uint32_t	byte_cnt;
+	uint32_t	xfr_ctrl;
+};
+
+#define VSPA_MAX_ELD_FILENAME (256)
+struct vspa_startup {
+	uint32_t	cmd_buf_size;
+	uint32_t	cmd_buf_addr;
+	uint32_t	spm_buf_size;
+	uint32_t	spm_buf_addr;
+	uint8_t		cmd_dma_chan;
+	char		filename[VSPA_MAX_ELD_FILENAME];
+};
+
+struct vspa_versions {
+	uint32_t	vspa_hw_version;
+	uint32_t	ippu_hw_version;
+	uint32_t	vspa_sw_version;
+	uint32_t	ippu_sw_version;
+};
+
+struct vspa_hardware {
+	uint32_t	param0;
+	uint32_t	param1;
+	uint32_t	param2;
+	uint32_t	axi_data_width;
+	uint32_t	dma_channels;
+	uint32_t	gp_out_regs;
+	uint32_t	gp_in_regs;
+	uint32_t	dmem_bytes;
+	uint32_t	ippu_bytes;
+	uint32_t	arithmetic_units;
+};
+
+struct vspa_reg {
+	uint32_t	reg;
+	uint32_t	val;
+};
+
+struct vspa_mb32 {
+	union {
+	  uint32_t      control;
+	  struct {
+	    uint8_t     id;
+	    uint8_t     flags;
+	    uint8_t     rsvd0;
+	    uint8_t     rsvd1;
+	  };
+	};
+	uint32_t        data;
+};
+
+struct vspa_mb64 {
+	union {
+	  uint32_t      control;
+	  struct {
+	    uint8_t     id;
+	    uint8_t     flags;
+	    uint8_t     rsvd0;
+	    uint8_t     rsvd1;
+	  };
+	};
+	uint32_t        data_msb;
+	uint32_t        data_lsb;
+};
+
+struct vspa_event_read {
+	uint32_t	event_mask;
+	int		timeout;
+	size_t		buf_len;
+	struct vspa_event *buf_ptr;
+};
+
+/* get VSPA Hardware configuration */
+#define VSPA_IOC_GET_HW_CFG	_IOR(VSPA_MAGIC_NUM, 0, struct vspa_hardware)
+
+/* VSPA operational state */
+#define VSPA_IOC_GET_STATE	_IOR(VSPA_MAGIC_NUM, 1, int)
+
+/* Read register */
+#define VSPA_IOC_REG_READ	_IOR(VSPA_MAGIC_NUM, 2, struct vspa_reg)
+
+/* Write register */
+#define VSPA_IOC_REG_WRITE	_IOW(VSPA_MAGIC_NUM, 3, struct vspa_reg)
+
+/* VSPA HW and SW versions */
+#define VSPA_IOC_GET_VERSIONS	_IOR(VSPA_MAGIC_NUM, 4, struct vspa_versions)
+
+/* Power Management request for vspa */
+#define VSPA_IOC_REQ_POWER	_IO(VSPA_MAGIC_NUM, 5)
+/* DMA transaction */
+#define VSPA_IOC_DMA		_IOW(VSPA_MAGIC_NUM, 6, struct vspa_dma_req)
+
+/* Startup VSPA core */
+#define VSPA_IOC_STARTUP	_IOW(VSPA_MAGIC_NUM, 7, struct vspa_startup)
+
+/* Write Mailbox transactions */
+#define VSPA_IOC_MB32_WRITE	_IOW(VSPA_MAGIC_NUM, 8, struct vspa_mb32)
+#define VSPA_IOC_MB64_WRITE	_IOW(VSPA_MAGIC_NUM, 9, struct vspa_mb64)
+
+/* Set Watchdog interval */
+#define VSPA_IOC_WATCHDOG_INT	_IO(VSPA_MAGIC_NUM, 10)
+#define VSPA_WATCHDOG_INTERVAL_DEFAULT	 (1000)
+#define VSPA_WATCHDOG_INTERVAL_MIN	  (100)
+#define VSPA_WATCHDOG_INTERVAL_MAX	(60000)
+
+/* Set the event mask used for poll checks */
+#define VSPA_IOC_SET_POLL_MASK	_IO(VSPA_MAGIC_NUM, 11)
+
+/* Retrieve the next matching event */
+#define VSPA_IOC_EVENT_READ	_IOW(VSPA_MAGIC_NUM, 12, struct vspa_event_read)
+
+/* Driver debug value */
+#define VSPA_IOC_GET_DEBUG	_IOR(VSPA_MAGIC_NUM, 13, int)
+#define VSPA_IOC_SET_DEBUG	_IO(VSPA_MAGIC_NUM, 14)
+
+#define VSPA_IOC_MAX (14)
+
+#endif /* _UAPI_VSPA_H */
-- 
2.7.4

